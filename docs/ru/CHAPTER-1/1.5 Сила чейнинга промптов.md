# 1.5 Сила чейнинга промптов

Чейнинг промптов — это способ решать сложные задачи через последовательность простых, взаимосвязанных шагов. Вместо одного «монолитного» запроса вы строите цепочку маленьких промптов: каждый шаг решает конкретную подзадачу и готовит контекст для следующего. Такой подход снижает вероятность ошибок, делает поведение модели управляемым и повышает наблюдаемость процесса: легче понять, где и почему возникла неточность, и вмешаться адресно. По сути это то же, что поэтапная готовка сложного блюда или модульная архитектура в разработке ПО — всегда проще отладить и сопровождать набор небольших, чётких операций, чем «спагетти» из одного громоздкого шага. Практические выгоды очевидны: удобнее управлять воркфлоу, фиксируя состояние на каждом этапе и адаптируя следующий шаг под результат предыдущего; экономить контекст и бюджет, потому что длинные промпты дороже, а в цепочке на каждом ходе используется лишь необходимый минимум; снижать число ошибок за счёт точной локализации проблемы; дозагружать только релевантную информацию, учитывая лимиты контекста LLM. Методологически это означает декомпозицию задачи на логичные подшаги, явное управление состоянием между шагами, предельно сфокусированный дизайн каждого промпта, добавление инструментов для загрузки и предобработки данных и динамическую подстановку лишь тех фрагментов контекста, которые нужны прямо сейчас. Лучшие практики просты: не переусложняйте там, где хватает одного промпта; формулируйте ясно; ведите и обновляйте внешний контекст; думайте об эффективности (качество, стоимость, задержка) и тестируйте цепочку как целое.

Ниже — последовательный пример, в котором мы шаг за шагом собираем сквозной сценарий с извлечением сущностей, обращением к простой «базе данных», парсингом JSON и формированием ответа для пользователя, а затем объединяем всё в единую историю обслуживания.

```python
import os
from openai import OpenAI
from dotenv import load_dotenv, find_dotenv

_ = load_dotenv(find_dotenv())
client = OpenAI()
```

Теперь извлечём сущности из пользовательского запроса. На первом шаге системная инструкция задаёт задачу и формат вывода, пользовательский ввод ограничен разделителями — так проще контролировать границы данных и передавать результат дальше по цепочке.

```python
def retrieve_model_response(message_sequence, model="gpt-4o-mini", temperature=0, max_tokens=500):
    response = client.chat.completions.create(
        model=model,
        messages=message_sequence,
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message["content"]

system_instruction = """
Вам будут даны запросы службы поддержки. Запрос ограничен '####'.
Верните Python-список объектов, каждый из которых — продукт или категория из запроса.
"""

user_query = "#### Расскажите мне о SmartX ProPhone и FotoSnap DSLR Camera, а также о ваших телевизорах ####"

message_sequence = [
    {'role': 'system', 'content': system_instruction},
    {'role': 'user', 'content': user_query},
]

extracted_info = retrieve_model_response(message_sequence)
print(extracted_info)
```

Далее подключим источник данных и найдём конкретные товары или категории. Даже простая «база» в памяти демонстрирует идею: извлекли сущности — перешли к структурированным данным — подготовили фактуру для ответа.

```python
product_database = {
    "TechPro Ultrabook": {
        "name": "TechPro Ultrabook",
        "category": "Computers and Laptops",
    },
    # ...
}

def get_product_details_by_name(product_name):
    return product_database.get(product_name, None)

def get_products_in_category(category_name):
    return [p for p in product_database.values() if p["category"] == category_name]

print(get_product_details_by_name("TechPro Ultrabook"))
print(get_products_in_category("Computers and Laptops"))
```

На следующем шаге преобразуем JSON‑строки, которые модель может вернуть в ответ на извлечение сущностей, в питоновские объекты, чтобы с ними удобно работать в последующих шагах чейнинга.

```python
import json

def json_string_to_python_list(json_string):
    if json_string is None:
        return None
    try:
        json_string = json_string.replace("'", '"')
        return json.loads(json_string)
    except json.JSONDecodeError:
        print("Error: Invalid JSON string")
        return None

json_input = "[{'category': 'Smartphones and Accessories', 'products': ['SmartX ProPhone']}]"
python_list = json_string_to_python_list(json_input)
print(python_list)
```

И, наконец, сформируем лаконичный ответ для пользователя из полученных структур. Такой слой форматирования легко заменить на шаблоны, локализацию или генерацию с учётом требований UX.

```python
def generate_response_from_data(product_data_list):
    response_string = ""
    if product_data_list is None:
        return response_string
    for data in product_data_list:
        response_string += json.dumps(data, indent=4) + "\n"
    return response_string

response_instruction = """
Вы — ассистент поддержки. Отвечайте кратко и предлагайте уточняющие вопросы при необходимости.
"""

final_response = generate_response_from_data(python_list)
print(final_response)
```

Завершим цельным сценарием обслуживания: сначала распознаём интерес к фототехнике, затем отвечаем по устранению неполадок, уточняем условия гарантии и завершаем рекомендациями по аксессуарам — четыре шага одной цепочки, каждый из которых опирается на результат предыдущего.

```python
system_instruction = """
Вам будут даны запросы службы поддержки, ограниченные '####'.
Верните список объектов: упомянутые продукты/категории.
"""

user_query_1 = "#### Я заинтересован в обновлении моего фотооборудования. Можете рассказать о последних DSLR камерах и совместимых аксессуарах? ####"

message_sequence_1 = [
    {'role': 'system', 'content': system_instruction},
    {'role': 'user', 'content': user_query_1},
]
response_1 = retrieve_model_response(message_sequence_1)
print("Ответ на запрос о товарах:", response_1)

troubleshooting_query = "#### Я только что купил FotoSnap DSLR Camera, которую вы порекомендовали, но у меня проблемы с подключением к смартфону. Что мне делать? ####"
system_instruction_troubleshooting = "Дайте пошаговые советы по устранению неполадок для проблемы клиента."
message_sequence_2 = [
    {'role': 'system', 'content': system_instruction_troubleshooting},
    {'role': 'user', 'content': troubleshooting_query},
]
response_2 = retrieve_model_response(message_sequence_2)
print("Ответ по устранению неполадок:", response_2)

follow_up_query = "#### Также, можете уточнить, что покрывает гарантия для FotoSnap DSLR Camera? ####"
system_instruction_follow_up = "Предоставьте подробную информацию о покрытии гарантии продукта."
message_sequence_3 = [
    {'role': 'system', 'content': system_instruction_follow_up},
    {'role': 'user', 'content': follow_up_query},
]
response_3 = retrieve_model_response(message_sequence_3)
print("Ответ с информацией о гарантии:", response_3)

additional_assistance_query = "#### Учитывая ваш интерес к фотографии, хотели бы вы получить рекомендации по объективам и штативам, совместимым с FotoSnap DSLR Camera? ####"
system_instruction_additional_assistance = "Предложите рекомендации по аксессуарам, которые дополняют существующие продукты пользователя."
message_sequence_4 = [
    {'role': 'system', 'content': system_instruction_additional_assistance},
    {'role': 'user', 'content': additional_assistance_query},
]
response_4 = retrieve_model_response(message_sequence_4)
print("Ответ с дополнительной помощью:", response_4)
```

В итоге цепочка промптов даёт устойчивый и понятный рабочий процесс: мы экономим контекст и бюджет, точнее локализуем ошибки и сохраняем гибкость ответа под задачу пользователя.

## Теоретические вопросы
1. Что такое чейнинг промптов и чем он отличается от использования одного длинного промпта?
2. Приведите две аналогии и объясните, как они соотносятся с чейнингом.
3. Как чейнинг промптов помогает управлять рабочим процессом (workflow)?
4. В чём заключается экономия при использовании чейнинга?
5. Как чейнинг промптов снижает количество ошибок при решении сложных задач?
6. Зачем нужна динамическая дозагрузка информации при ограниченном контексте LLM?
7. Опишите пошаговую методологию внедрения чейнинга и назначение каждого шага.
8. Перечислите лучшие практики для обеспечения эффективности чейнинга.
9. Какие библиотеки используются в примере и для каких целей?
10. Как системное сообщение направляет ответ модели?
11. Какова роль базы данных товаров в примере, и как из неё получать данные?
12. Зачем требуется преобразовывать JSON-строки в Python-объекты, и как это сделать?
13. Как формирование ответа из обработанных данных повышает качество сервиса?
14. Как сквозной сценарий демонстрирует адаптацию к потребностям пользователя через чейнинг промптов?

## Практические задания
1. Реализуйте функцию `retrieve_model_response` с параметрами `model`, `temperature` и `max_tokens`.
2. Покажите пример извлечения сущностей из запроса с помощью системной инструкции.
3. Создайте мини-базу товаров и функции для поиска по имени или категории.
4. Реализуйте конвертацию JSON-строки в Python-список с обработкой ошибок.
5. Напишите функцию `generate_response_from_data`, которая форматирует список данных в удобный для пользователя ответ.
6. Скомпонуйте сквозной сценарий обслуживания (запрос → диагностика → гарантия → рекомендации) на базе вышеописанных функций.
