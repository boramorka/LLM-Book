
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.17">
    
    
      
        <title>1.4 Elevating Machine Reasoning: Advanced Strategies - LLMOps. Make AI Work For You.</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#14-elevating-machine-reasoning-advanced-strategies" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="LLMOps. Make AI Work For You." class="md-header__button md-logo" aria-label="LLMOps. Make AI Work For You." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LLMOps. Make AI Work For You.
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              1.4 Elevating Machine Reasoning: Advanced Strategies
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="Select language">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="../../en/" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="../../ru/" hreflang="ru" class="md-select__link">
              Русский
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/boramorka/LLM-book" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    boramorka/LLM-book
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="LLMOps. Make AI Work For You." class="md-nav__button md-logo" aria-label="LLMOps. Make AI Work For You." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    LLMOps. Make AI Work For You.
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/boramorka/LLM-book" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    boramorka/LLM-book
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="14-elevating-machine-reasoning-advanced-strategies">1.4 Elevating Machine Reasoning: Advanced Strategies</h1>
<h2 id="chain-of-thought-reasoning">Chain of Thought Reasoning</h2>
<p>Chain of Thought Reasoning represents a sophisticated method of enhancing artificial intelligence models, particularly those involved in problem-solving tasks. By breaking down the problem-solving process into a sequence of logical steps, this approach enables the model to navigate through complex queries with greater precision and methodical strategy. This step-by-step decomposition of the thought process is crucial for tackling problems that require deep understanding and nuanced interpretation. The primary advantage of Chain of Thought Reasoning is twofold: firstly, it significantly boosts the model's accuracy by ensuring that each step of the problem-solving process is grounded in logic and clear reasoning. Secondly, it demystifies the model's internal decision-making process, making it easier for users to follow and understand how a particular conclusion was reached. This transparency is key to building trust and enhancing the user experience, as it allows users to see the "thought process" behind the model's responses, much like following the reasoning of a human expert.</p>
<p>Applications of Chain of Thought Reasoning are vast and varied, reflecting its versatility and effectiveness across different domains.</p>
<p><strong>Tutoring and Educational Tools</strong></p>
<p>In the realm of education, Chain of Thought Reasoning can revolutionize the way tutoring and educational tools are designed and utilized. By integrating this reasoning approach, educational software can simulate the thought process of an expert tutor, guiding students through complex problems step by step. This method is particularly beneficial because it encourages active learning; students are not merely presented with answers but are led to understand the logical progression that leads to those answers. Such an approach fosters deeper comprehension, critical thinking skills, and the ability to apply concepts in varied contexts. Importantly, it supports the educational ethos of teaching students to "fish" rather than just "giving them a fish," thereby equipping them with lifelong learning skills.</p>
<p><strong>Customer Service Bots</strong></p>
<p>In the domain of customer service, Chain of Thought Reasoning can significantly enhance the effectiveness of bots and automated support systems. Traditional customer service bots often struggle with understanding and accurately responding to complex or nuanced customer queries. By applying Chain of Thought Reasoning, these bots can better comprehend the customer's issue and navigate through a logical series of steps to provide a more accurate, helpful response. This not only improves customer satisfaction but also reduces the need for human intervention, making customer support operations more efficient and scalable. Additionally, by making the reasoning process transparent, customers can understand the logic behind the bot's responses, leading to a more satisfying and reassuring interaction.</p>
<p>In summary, Chain of Thought Reasoning stands as a transformative approach in the development of AI models, with significant implications for educational tools and customer service operations. Its ability to break down complex problems into understandable logical steps not only enhances model accuracy but also fosters a more engaging and transparent interaction between AI systems and their human users.</p>
<h2 id="inner-monologue-technique">Inner Monologue Technique</h2>
<p>The Inner Monologue Technique represents a nuanced approach in the design and operation of artificial intelligence models, particularly focusing on how these models communicate their reasoning and conclusions to users. Unlike the Chain of Thought Reasoning, which aims to make the model's thought process transparent, the Inner Monologue Technique takes a more reserved approach. It involves the model processing and considering various steps and elements internally without exposing this entire process to the user. Instead, only the final output or selected aspects of the reasoning that are deemed relevant and helpful are shared. This technique is especially valuable in contexts where detailed exposure of the thought process could detract from the user experience, overwhelm the user with unnecessary information, or, more critically, compromise privacy or lead to the disclosure of sensitive information.</p>
<p><strong>Sensitive Information Filtering</strong></p>
<p>One of the key applications of the Inner Monologue Technique is in the domain of sensitive information filtering. In many scenarios, AI models are required to process and analyze data that may include personal, confidential, or sensitive information. Exposing the entire thought process and all the data considered by the model could potentially lead to privacy breaches or unwanted disclosure of sensitive information. By employing the Inner Monologue Technique, AI systems can ensure that they only display content that is appropriate and has been deemed safe for sharing with the user. This careful selection process protects user privacy and maintains the integrity and confidentiality of the information being processed.</p>
<p><strong>Guided Learning Applications</strong></p>
<p>Another critical application of this technique is in guided learning applications and educational tools. In learning environments, it is often beneficial to challenge students and encourage them to arrive at answers through their reasoning and problem-solving skills. Revealing the full reasoning process or the answer too early can undermine this learning process, making it less effective. The Inner Monologue Technique allows educational AI systems to guide learners towards the correct answer by providing hints or showing partial reasoning steps without giving away the entire solution. This approach maintains a balance between offering necessary guidance and ensuring that learners engage deeply with the material, fostering a more robust understanding and retention of knowledge.</p>
<p>In summary, the Inner Monologue Technique offers a strategic approach to managing how much of the AI's reasoning process is revealed to users. By selectively hiding certain parts of the model's thought process, this technique can enhance user experience in scenarios where full transparency is not desirable or necessary. Whether it's protecting sensitive information or creating more effective learning experiences, the Inner Monologue Technique provides a valuable tool in the design and implementation of artificial intelligence systems, ensuring they operate in a manner that is both responsible and aligned with the specific needs of their application contexts.</p>
<h2 id="example">Example</h2>
<h3 id="setting-up-the-environment">Setting Up the Environment</h3>
<p>Before diving into the implementation, it's crucial to set up the necessary environment. This includes loading the OpenAI API key and importing relevant Python libraries. The following code block demonstrates how to achieve this setup, ensuring that your environment is prepared for executing the subsequent reasoning tasks.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Import necessary libraries</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dotenv</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dotenv</span><span class="p">,</span> <span class="n">find_dotenv</span>

<span class="c1"># Load environment variables, specifically the OpenAI API key</span>
<span class="n">load_dotenv</span><span class="p">(</span><span class="n">find_dotenv</span><span class="p">())</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
</code></pre></div>
<h3 id="defining-the-function-for-processing-inputs">Defining the Function for Processing Inputs</h3>
<p>The core function <code>get_response_for_queries</code> retrieves responses from the model based on a structured series of prompts. This function encapsulates the logic for sending queries to the OpenAI API and parsing the responses.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_response_for_queries</span><span class="p">(</span><span class="n">query_prompts</span><span class="p">,</span>
                             <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
                             <span class="n">response_temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_response_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Retrieve model responses for a list of query prompts.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - query_prompts: A list containing the system and user prompts.</span>
<span class="sd">    - model_name: Specifies the model version to use.</span>
<span class="sd">    - response_temperature: Controls the randomness of the model&#39;s responses.</span>
<span class="sd">    - max_response_tokens: Limits the length of the model&#39;s response.</span>

<span class="sd">    Returns:</span>
<span class="sd">    The model&#39;s response to the user&#39;s query.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model_response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="n">query_prompts</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">response_temperature</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_response_tokens</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model_response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="p">[</span><span class="s2">&quot;content&quot;</span><span class="p">]</span>
</code></pre></div>
<h3 id="chain-of-thought-prompting">Chain-of-Thought Prompting</h3>
<p>Chain-of-Thought prompting is a technique that guides the model through a structured reasoning process before arriving at a final response. This method is especially useful for complex queries where direct answers are not readily apparent.</p>
<p><strong>Example: Structuring System and User Prompts</strong></p>
<p>To illustrate the application of Chain-of-Thought prompting, consider the task of providing detailed product information in response to customer inquiries. The example below outlines how to structure both the system and user prompts to facilitate this process.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Define a delimiter for separating reasoning steps</span>
<span class="n">step_delimiter</span> <span class="o">=</span> <span class="s2">&quot;####&quot;</span>

<span class="c1"># System prompt guiding the model through the reasoning process</span>
<span class="n">system_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Follow these steps to answer customer queries, using &#39;</span><span class="si">{</span><span class="n">step_delimiter</span><span class="si">}</span><span class="s2">&#39; to delineate each step.</span>

<span class="s2">Step 1:</span><span class="si">{</span><span class="n">step_delimiter</span><span class="si">}</span><span class="s2"> Determine if the query pertains to a specific product rather than a general category.</span>

<span class="s2">Step 2:</span><span class="si">{</span><span class="n">step_delimiter</span><span class="si">}</span><span class="s2"> Identify if the product is among the listed items, including details such as brand, features, and price.</span>

<span class="s2">[Provide a list of products here]</span>

<span class="s2">Step 3:</span><span class="si">{</span><span class="n">step_delimiter</span><span class="si">}</span><span class="s2"> Assess any assumptions made by the customer regarding product comparisons or specifications.</span>

<span class="s2">Step 4:</span><span class="si">{</span><span class="n">step_delimiter</span><span class="si">}</span><span class="s2"> Verify the accuracy of these assumptions based on provided product information.</span>

<span class="s2">Step 5:</span><span class="si">{</span><span class="n">step_delimiter</span><span class="si">}</span><span class="s2"> Correct any misconceptions, referencing only the listed products, and respond in a courteous manner.</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="c1"># Example user queries</span>
<span class="n">example_query_1</span> <span class="o">=</span> <span class="s2">&quot;How does the BlueWave Chromebook compare to the TechPro Desktop in terms of cost?&quot;</span>
<span class="n">example_query_2</span> <span class="o">=</span> <span class="s2">&quot;Are televisions available for sale?&quot;</span>

<span class="c1"># Formulating query prompts for the model</span>
<span class="n">query_prompts_1</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">system_prompt</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">step_delimiter</span><span class="si">}{</span><span class="n">example_query_1</span><span class="si">}{</span><span class="n">step_delimiter</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">},</span>
<span class="p">]</span>

<span class="n">query_prompts_2</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">system_prompt</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">step_delimiter</span><span class="si">}{</span><span class="n">example_query_2</span><span class="si">}{</span><span class="n">step_delimiter</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">},</span>
<span class="p">]</span>
</code></pre></div>
<h3 id="retrieving-and-presenting-model-responses">Retrieving and Presenting Model Responses</h3>
<p>After structuring the prompts, the next step involves querying the model and extracting the relevant parts of its responses. This is crucial for presenting the final answer to the user in a concise and clear manner.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Retrieve the model&#39;s response for the first example query</span>
<span class="n">response_to_query_1</span> <span class="o">=</span> <span class="n">get_response_for_queries</span><span class="p">(</span><span class="n">query_prompts_1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response_to_query_1</span><span class="p">)</span>

<span class="c1"># Retrieve the model&#39;s response for the second example query</span>
<span class="n">response_to_query_2</span> <span class="o">=</span> <span class="n">get_response_for_queries</span><span class="p">(</span><span class="n">query_prompts_2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response_to_query_2</span><span class="p">)</span>
</code></pre></div>
<h3 id="implementing-inner-monologue">Implementing Inner Monologue</h3>
<p>The Inner Monologue technique selectively presents the final answer while excluding the intermediate steps of reasoning. This ensures users get direct answers without the complexity of the model's thought process.</p>
<p>The code snippet provided demonstrates how to extract and handle the final response from a model's output in a scenario where the output is structured using a specific delimiter to separate reasoning steps. This technique is especially useful when implementing the Inner Monologue technique, ensuring that users are presented with only the essential conclusion of the model's processing. Let's break down the code and its functionality in more detail:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Extracting only the final response from the model&#39;s output</span>
<span class="k">try</span><span class="p">:</span>
    <span class="c1"># The response from the model is assumed to be in a variable named &#39;response_to_query_2&#39;</span>
    <span class="c1"># The &#39;split&#39; method is used to divide the output into a list of segments based on the &#39;step_delimiter&#39;</span>
    <span class="c1"># The &#39;[-1]&#39; selects the last item in this list, which is the final response after all reasoning steps</span>
    <span class="n">final_response</span> <span class="o">=</span> <span class="n">response_to_query_2</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">step_delimiter</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="c1"># If any error occurs during the process (e.g., &#39;response_to_query_2&#39; is not defined), </span>
    <span class="c1"># a default error message is assigned to &#39;final_response&#39;</span>
    <span class="n">final_response</span> <span class="o">=</span> <span class="s2">&quot;Sorry, I&#39;m having trouble right now, please try asking another question.&quot;</span>
</code></pre></div>
<p>The <code>try</code> block attempts to execute the code that extracts the final response. The model's output, stored in <code>response_to_query_2</code>, is split into segments using <code>step_delimiter</code> as the dividing point. By accessing the last segment (<code>[-1]</code>), we ensure that we're only capturing the conclusion of the model's reasoning process. The <code>strip()</code> method is applied to remove any leading or trailing whitespace, ensuring the final response is neatly formatted.</p>
<p>The <code>except</code> block is a safety net that catches any exceptions that might occur during the extraction process. Exceptions can arise for various reasons, such as if <code>response_to_query_2</code> is undefined or if the string does not contain the delimiter, leading to an index error. In such cases, rather than allowing the program to crash or expose the user to a raw error message, a friendly, predefined response is returned, maintaining a smooth user experience.</p>
<p>Finally, <code>print(final_response)</code> is used to display the final response to the user. This approach ensures that the user receives a clear and concise answer, aligning with the principles of the Inner Monologue technique by omitting unnecessary details about the model's internal thought process.</p>
<p>This method enhances the user interface by ensuring that the system's responses are both user-friendly and focused on providing relevant information directly, without overwhelming users with the complexities of the background processing.</p>
<h2 id="conclusion-and-best-practices">Conclusion and Best Practices</h2>
<p>Implementing Chain of Thought Reasoning and Inner Monologue with OpenAI's API enhances the model's ability to process complex queries methodically, providing clear and accurate responses. When integrating these techniques:</p>
<ul>
<li>Ensure clarity in prompt structuring to guide the model effectively.</li>
<li>Regularly refine prompts based on the model's performance to optimize response quality.</li>
<li>Consider user experience by presenting responses in a straightforward manner, using techniques like Inner Monologue to streamline information delivery.</li>
</ul>
<p>For further exploration of advanced model interaction techniques and prompt engineering, refer to OpenAI's documentation and relevant academic literature in the field of natural language processing and machine learning.</p>
<h2 id="theory-questions">Theory questions:</h2>
<ol>
<li>What is Chain of Thought Reasoning and how does it enhance artificial intelligence models in problem-solving tasks?</li>
<li>How does the transparency provided by Chain of Thought Reasoning benefit users and build trust in AI models?</li>
<li>Describe the role of Chain of Thought Reasoning in educational tools. How does it improve the learning experience for students?</li>
<li>How can Chain of Thought Reasoning improve customer satisfaction in customer service bots?</li>
<li>What is the Inner Monologue Technique and how does it differ from Chain of Thought Reasoning in terms of information presentation to the user?</li>
<li>Discuss the importance of the Inner Monologue Technique in the context of sensitive information filtering.</li>
<li>How does the Inner Monologue Technique benefit guided learning applications without compromising the learning process?</li>
<li>Explain the process of setting up the environment for implementing Chain of Thought Reasoning and Inner Monologue techniques using Python and OpenAI API.</li>
<li>Describe the function of <code>get_response_for_queries</code> in processing inputs for AI models.</li>
<li>How does the Chain-of-Thought prompting technique facilitate handling complex queries?</li>
<li>In the context of customer service, explain how structuring system and user prompts can aid in providing detailed product information.</li>
<li>How is the final response extracted and presented in the Inner Monologue implementation, and why is this approach beneficial for user experience?</li>
</ol>
<h2 id="practice-questions">Practice questions:</h2>
<ol>
<li>
<p>Implement a function <code>chain_of_thought_prompting</code> in Python that takes a user query as input and generates a structured system prompt based on the steps provided in the "Chain-of-Thought Prompting" section. The function should then return the system prompt and the structured user query as two separate strings.</p>
</li>
<li>
<p>Write a Python function <code>get_final_response</code> that extracts only the final response from a model's output, assuming the output is structured using a specific delimiter to separate reasoning steps. The function should take the model's output and the delimiter as inputs and return the final response. Handle any potential errors gracefully, returning a predefined error message if the extraction process fails.</p>
</li>
<li>
<p>Create a Python script that uses the <code>get_response_for_queries</code> function to send two different types of queries to the OpenAI API: one that requires Chain of Thought Reasoning and another that is best suited for the Inner Monologue Technique. Use the example queries provided in the "Chain-of-Thought Prompting" and "Implementing Inner Monologue" sections for this task. The script should print out the responses for both queries.</p>
</li>
<li>
<p>Design a Python function <code>validate_response_structure</code> that checks if a response from the model correctly follows the structure defined by the Chain of Thought Reasoning steps. The function should accept the model's response and the step delimiter as inputs, returning <code>True</code> if the response adheres to the expected structure (i.e., contains the specified number of reasoning steps) and <code>False</code> otherwise.</p>
</li>
<li>
<p>Develop a Python class <code>QueryProcessor</code> that encapsulates the functionality for both Chain of Thought Reasoning and Inner Monologue techniques. The class should have methods for setting up the environment (loading API keys), structuring prompts, sending queries, and processing responses. Include error handling to manage issues such as network failures or API limits.</p>
</li>
</ol>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
    
  </body>
</html>