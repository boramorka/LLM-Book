
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../Answers%202.3/">
      
      
        <link rel="next" href="../Answers%202.5/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.12">
    
    
      
        <title>2.4 The Power of Embeddings - LLMOps. Make AI Work For You.</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.7e359304.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#answers-24" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="LLMOps. Make AI Work For You." class="md-header__button md-logo" aria-label="LLMOps. Make AI Work For You." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LLMOps. Make AI Work For You.
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2.4 The Power of Embeddings
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/boramorka/LLM-book" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    boramorka/LLM-book
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="LLMOps. Make AI Work For You." class="md-nav__button md-logo" aria-label="LLMOps. Make AI Work For You." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    LLMOps. Make AI Work For You.
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/boramorka/LLM-book" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    boramorka/LLM-book
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    CHAPTER-1. OPEN AI API
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            CHAPTER-1. OPEN AI API
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/1.1%20Introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.1 Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/1.2%20Classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.2 Classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/1.3%20Advanced%20Moderaton/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.3 Advanced Moderaton
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/1.4%20Elevating%20Machine%20Reasoning%3A%20Advanced%20Strategies/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.4 Elevating Machine Reasoning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/1.5%20The%20Power%20of%20Prompt%20Chaining/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.5 The Power of Prompt Chaining
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/1.6%20Building%20and%20Evaluating%20LLM%20Applications/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.6 Building and Evaluating LLM Applications
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/1.7%20Summary%20and%20Reflections/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.7 Summary and Reflections
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    CHAPTER-2. LANGCHAIN
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            CHAPTER-2. LANGCHAIN
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2.1%20Introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2.1 Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2.2%20LangChain%20Document%20Loaders/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2.2 LangChain Document Loaders
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2.3%20Deep%20Dive%20into%20Text%20Splitting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2.3 Deep Dive into Text Splitting
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2.4%20The%20Power%20of%20Embeddings/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2.4 The Power of Embeddings
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2.5%20Semantic%20Search.%20Advanced%20Retrieval%20Strategies/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2.5 Semantic Search. Advanced Retrieval Strategies
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2.6%20RAG%20Systems.%20Techniques%20for%20Question%20Answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2.6 RAG Systems. Techniques for Question Answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2.7%20Building%20Chatbots%20with%20LangChain/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2.7 Building Chatbots with LangChain
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2.8%20Summary%20and%20Reflections/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2.8 Summary and Reflections
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  <span class="md-ellipsis">
    CHAPTER-3. LLMOPS
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            CHAPTER-3. LLMOPS
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-3/3.1%20Introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3.1 Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-3/3.2%20Mastering%20LLM%20Workflows%20with%20Kubeflow%20Pipelines/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3.2 Mastering LLM Workflows with Kubeflow Pipelines
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-3/3.3%20Implementing%20the%20AI%20Quiz%20Generation%20Mechanism/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3.3 Implementing the AI Quiz Generation Mechanism
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-3/3.4%20Summary%20and%20Reflections/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3.4 Summary and Reflections
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Answers
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Answers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/Answers%201.1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.1 Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/Answers%201.2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.2 Classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/Answers%201.3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.3 Advanced Moderaton
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/Answers%201.4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.4 Elevating Machine Reasoning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/Answers%201.5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.5 The Power of Prompt Chaining
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/Answers%201.6/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.6 Building and Evaluating LLM Applications
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Answers%202.2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2.2 LangChain Document Loaders
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Answers%202.3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2.3 Deep Dive into Text Splitting
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    2.4 The Power of Embeddings
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    2.4 The Power of Embeddings
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#theory" class="md-nav__link">
    <span class="md-ellipsis">
      Theory
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#practice" class="md-nav__link">
    <span class="md-ellipsis">
      Practice
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Answers%202.5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2.5 Semantic Search. Advanced Retrieval Strategies
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Answers%202.6/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2.6 RAG Systems. Techniques for Question Answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Answers%202.7/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2.7 Building Chatbots with LangChain
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-3/Answers%203.2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3.2 Mastering LLM Workflows with Kubeflow Pipelines
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-3/Answers%203.3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3.3 Implementing the AI Quiz Generation Mechanism
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="answers-24">Answers 2.4</h1>
<h2 id="theory">Theory</h2>
<ol>
<li>The primary purpose of converting textual information into embeddings is to transform text into numerical vectors in a way that captures semantic meaning, enabling computers to process and understand textual data more effectively.</li>
<li>Embeddings capture semantic similarity by positioning words or sentences with similar meanings closer to each other in a high-dimensional vector space, facilitating tasks like semantic search.</li>
<li>The process of creating word embeddings involves training models on large text corpora to learn vector representations of words based on their context, ensuring that words used in similar contexts have similar vector representations.</li>
<li>In semantic search, embeddings allow the system to understand the intent and contextual meaning behind queries, enabling it to retrieve documents that are semantically related to the query, even without exact keyword matches.</li>
<li>Document embeddings represent the semantic essence of entire documents, while query embeddings represent the semantic intent of search queries. Comparing these embeddings enables the identification of documents that are semantically relevant to the query.</li>
<li>A vector store is a database optimized for storing and retrieving high-dimensional vector data (embeddings), crucial for performing efficient similarity searches in applications like semantic search.</li>
<li>When choosing a vector store, considerations include the size of the dataset, persistence requirements, and the specific use case, such as whether the application is for research, development, or production use.</li>
<li>Chroma is suitable for rapid prototyping and small datasets due to its in-memory nature, which allows for fast data retrieval. Its limitations include lack of persistence and scalability for larger datasets.</li>
<li>The workflow involves document splitting, embedding generation, vector store indexing, query processing, and response generation, collectively enabling efficient semantic search capabilities.</li>
<li>Document splitting improves search granularity and relevance by breaking down documents into semantically coherent chunks, allowing for more precise matching of document parts to queries.</li>
<li>Embedding generation for document chunks involves mapping text to high-dimensional vectors, capturing the semantic features of the text and enabling efficient computational processing and comparison.</li>
<li>Vector store indexing allows for the efficient storage and retrieval of embeddings, enabling quick similarity searches to find document chunks most relevant to a given query.</li>
<li>Query processing involves generating an embedding for the user's query and searching the vector store for document embeddings that are most similar, using metrics like Euclidean distance or cosine similarity.</li>
<li>Response generation enhances user experience by using retrieved document chunks and the original query to generate coherent and contextually relevant responses, leveraging large language models.</li>
<li>Setting up the environment involves importing necessary libraries, setting up API keys, and configuring the system to ensure it is properly prepared for embedding and vector store operations.</li>
<li>Document loading and splitting are crucial for managing textual data more effectively, breaking it down into smaller, manageable, and semantically meaningful chunks for better processing.</li>
<li>Generating embeddings transforms textual information into numerical vectors that encapsulate semantic meanings, with similarity demonstrated through metrics like the dot product between vectors.</li>
<li>When setting up Chroma, considerations include the directory for persistence, clearing existing data for a fresh start, and initializing the store with document splits and embeddings for retrieval.</li>
<li>Similarity searches facilitate the retrieval of relevant document chunks by comparing the query embedding with document embeddings to find the closest matches based on semantic similarity.</li>
<li>Potential failure modes in semantic searches include duplicate entries and irrelevant document inclusion. Addressing these involves refining the search process to ensure relevance and distinctiveness of results.</li>
</ol>
<h2 id="practice">Practice</h2>
<ol>
<li>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">generate_embeddings</span><span class="p">(</span><span class="n">sentences</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates a simple placeholder embedding for each sentence based on its length.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - sentences (list of str): A list of sentences to generate embeddings for.</span>

<span class="sd">    Returns:</span>
<span class="sd">    - list of int: A list of embeddings, where each embedding is the length of the corresponding sentence.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">vector_a</span><span class="p">,</span> <span class="n">vector_b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the cosine similarity between two vectors.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - vector_a (list of float): The first vector.</span>
<span class="sd">    - vector_b (list of float): The second vector.</span>

<span class="sd">    Returns:</span>
<span class="sd">    - float: The cosine similarity between vector_a and vector_b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dot_product</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">a</span><span class="o">*</span><span class="n">b</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">vector_a</span><span class="p">,</span> <span class="n">vector_b</span><span class="p">))</span>
    <span class="n">magnitude_a</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">a</span><span class="o">**</span><span class="mi">2</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">vector_a</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
    <span class="n">magnitude_b</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">b</span><span class="o">**</span><span class="mi">2</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">vector_b</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
    <span class="k">return</span> <span class="n">dot_product</span> <span class="o">/</span> <span class="p">(</span><span class="n">magnitude_a</span> <span class="o">*</span> <span class="n">magnitude_b</span><span class="p">)</span>

<span class="c1"># Example usage:</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Hello, world!&quot;</span><span class="p">,</span> <span class="s2">&quot;This is a longer sentence.&quot;</span><span class="p">,</span> <span class="s2">&quot;Short&quot;</span><span class="p">]</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">generate_embeddings</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Embeddings:&quot;</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>

<span class="n">vector_a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">vector_b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">similarity</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">vector_a</span><span class="p">,</span> <span class="n">vector_b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cosine Similarity:&quot;</span><span class="p">,</span> <span class="n">similarity</span><span class="p">)</span>
</code></pre></div>
</li>
<li>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">vector_a</span><span class="p">,</span> <span class="n">vector_b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculates the cosine similarity between two vectors.&quot;&quot;&quot;</span>
    <span class="n">dot_product</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">a</span><span class="o">*</span><span class="n">b</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">vector_a</span><span class="p">,</span> <span class="n">vector_b</span><span class="p">))</span>
    <span class="n">magnitude_a</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">a</span><span class="o">**</span><span class="mi">2</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">vector_a</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
    <span class="n">magnitude_b</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">b</span><span class="o">**</span><span class="mi">2</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">vector_b</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
    <span class="k">if</span> <span class="n">magnitude_a</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">magnitude_b</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>  <span class="c1"># Avoid division by zero</span>
    <span class="k">return</span> <span class="n">dot_product</span> <span class="o">/</span> <span class="p">(</span><span class="n">magnitude_a</span> <span class="o">*</span> <span class="n">magnitude_b</span><span class="p">)</span>
</code></pre></div>
</li>
<li>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">SimpleVectorStore</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vectors</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">add_vector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vector</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Adds a vector to the store.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vectors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">find_most_similar</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_vector</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Finds and returns the most similar vector to the query_vector.&quot;&quot;&quot;</span>
        <span class="n">similarities</span> <span class="o">=</span> <span class="p">[</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">query_vector</span><span class="p">,</span> <span class="n">vector</span><span class="p">)</span> <span class="k">for</span> <span class="n">vector</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vectors</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">similarities</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="n">max_index</span> <span class="o">=</span> <span class="n">similarities</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">similarities</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">vectors</span><span class="p">[</span><span class="n">max_index</span><span class="p">]</span>
</code></pre></div>
</li>
<li>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">sys</span>

<span class="k">def</span> <span class="nf">split_text_into_chunks</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Splits the given text into chunks of the specified size.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">text</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">chunk_size</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">),</span> <span class="n">chunk_size</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">load_and_print_chunks</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loads text from a file, splits it into chunks, and prints each chunk.&quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">text</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
            <span class="n">chunks</span> <span class="o">=</span> <span class="n">split_text_into_chunks</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chunks</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chunk </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">:</span><span class="se">\n</span><span class="si">{</span><span class="n">chunk</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;-&#39;</span><span class="o">*</span><span class="mi">50</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">FileNotFoundError</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error: File &#39;</span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">&#39; not found.&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;An unexpected error occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Usage: python script.py &lt;file_path&gt; &lt;chunk_size&gt;&quot;</span><span class="p">)</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">file_path</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">chunk_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">load_and_print_chunks</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">)</span>
</code></pre></div>
</li>
<li>
<div class="highlight"><pre><span></span><code><span class="c1"># Assuming the SimpleVectorStore and cosine_similarity function are defined as shown previously</span>

<span class="k">def</span> <span class="nf">generate_query_embedding</span><span class="p">(</span><span class="n">query</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generates a simple placeholder embedding for the query based on its length.&quot;&quot;&quot;</span>
    <span class="c1"># This is a placeholder. In a real scenario, you would use a model to generate embeddings.</span>
    <span class="k">return</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">query</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">query_processing</span><span class="p">(</span><span class="n">store</span><span class="p">,</span> <span class="n">query</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Processes a query by generating an embedding, searching the vector store, and printing the most similar document chunk.&quot;&quot;&quot;</span>
    <span class="n">query_embedding</span> <span class="o">=</span> <span class="n">generate_query_embedding</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">most_similar</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">find_most_similar</span><span class="p">(</span><span class="n">query_embedding</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">most_similar</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The most similar document chunk is:&quot;</span><span class="p">,</span> <span class="n">most_similar</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No document chunks found.&quot;</span><span class="p">)</span>
</code></pre></div>
</li>
<li>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">remove_duplicates</span><span class="p">(</span><span class="n">document_chunks</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Removes duplicate document chunks based on an exact match.&quot;&quot;&quot;</span>
    <span class="n">unique_chunks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">document_chunks</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">chunk</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">unique_chunks</span><span class="p">:</span>
            <span class="n">unique_chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">unique_chunks</span>
</code></pre></div>
</li>
<li>
<div class="highlight"><pre><span></span><code><span class="c1"># Initialization of the SimpleVectorStore</span>
<span class="n">store</span> <span class="o">=</span> <span class="n">SimpleVectorStore</span><span class="p">()</span>

<span class="c1"># Placeholder for document chunks and their embeddings</span>
<span class="n">document_chunks</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Document chunk 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Document chunk 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Document chunk 3&quot;</span><span class="p">]</span>
<span class="c1"># Simulating embeddings for these chunks as placeholders (for example, based on their length)</span>
<span class="n">document_embeddings</span> <span class="o">=</span> <span class="p">[[</span><span class="nb">len</span><span class="p">(</span><span class="n">chunk</span><span class="p">)]</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">document_chunks</span><span class="p">]</span>

<span class="c1"># Adding document embeddings to the store</span>
<span class="k">for</span> <span class="n">embedding</span> <span class="ow">in</span> <span class="n">document_embeddings</span><span class="p">:</span>
    <span class="n">store</span><span class="o">.</span><span class="n">add_vector</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>

<span class="c1"># Conducting a similarity search with a sample query</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;Document&quot;</span>
<span class="n">query_embedding</span> <span class="o">=</span> <span class="n">generate_query_embedding</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

<span class="c1"># Finding the most similar document chunks based on cosine similarity</span>
<span class="n">similarities</span> <span class="o">=</span> <span class="p">[(</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">query_embedding</span><span class="p">,</span> <span class="n">doc_embedding</span><span class="p">),</span> <span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">doc_embedding</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">document_embeddings</span><span class="p">)]</span>
<span class="n">similarities</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Sort by similarity in descending order</span>
<span class="n">top_n_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">similarities</span><span class="p">[:</span><span class="mi">3</span><span class="p">]]</span>  <span class="c1"># Get the indices of the top 3 most similar chunks</span>

<span class="c1"># Printing the IDs or contents of the top 3 most similar document chunks</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Top 3 most similar document chunks:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">top_n_indices</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">document_chunks</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
</li>
<li>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">embed_and_store_documents</span><span class="p">(</span><span class="n">document_chunks</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates embeddings for each document chunk and stores them in a SimpleVectorStore.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - document_chunks (list of str): A list of document chunks as strings.</span>

<span class="sd">    Returns:</span>
<span class="sd">    - SimpleVectorStore: The vector store initialized with document embeddings.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">store</span> <span class="o">=</span> <span class="n">SimpleVectorStore</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">document_chunks</span><span class="p">:</span>
        <span class="c1"># Placeholder for generating an embedding based on the chunk&#39;s length</span>
        <span class="n">embedding</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">chunk</span><span class="p">)]</span>
        <span class="n">store</span><span class="o">.</span><span class="n">add_vector</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">store</span>
</code></pre></div>
</li>
<li>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">json</span>

<span class="k">def</span> <span class="nf">save_vector_store</span><span class="p">(</span><span class="n">store</span><span class="p">,</span> <span class="n">filepath</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Saves the state of a SimpleVectorStore to a file.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - store (SimpleVectorStore): The vector store to save.</span>
<span class="sd">    - filepath (str): The path to the file where the store should be saved.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">store</span><span class="o">.</span><span class="n">vectors</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">load_vector_store</span><span class="p">(</span><span class="n">filepath</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Loads the state of a SimpleVectorStore from a file.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - filepath (str): The path to the file from which to load the store.</span>

<span class="sd">    Returns:</span>
<span class="sd">    - SimpleVectorStore: The loaded vector store.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">store</span> <span class="o">=</span> <span class="n">SimpleVectorStore</span><span class="p">()</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">store</span><span class="o">.</span><span class="n">vectors</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">store</span>

<span class="k">def</span> <span class="nf">vector_store_persistence</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrates saving and loading the state of a SimpleVectorStore.&quot;&quot;&quot;</span>
    <span class="n">store</span> <span class="o">=</span> <span class="n">SimpleVectorStore</span><span class="p">()</span>  <span class="c1"># Assume this is already populated</span>
    <span class="n">filepath</span> <span class="o">=</span> <span class="s1">&#39;vector_store.json&#39;</span>

    <span class="c1"># Example of saving and loading</span>
    <span class="n">save_vector_store</span><span class="p">(</span><span class="n">store</span><span class="p">,</span> <span class="n">filepath</span><span class="p">)</span>
    <span class="n">loaded_store</span> <span class="o">=</span> <span class="n">load_vector_store</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vector store loaded with vectors:&quot;</span><span class="p">,</span> <span class="n">loaded_store</span><span class="o">.</span><span class="n">vectors</span><span class="p">)</span>
</code></pre></div>
</li>
<li>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">evaluate_search_accuracy</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">expected_chunks</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluates the accuracy of similarity searches for a list of queries against expected results.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - queries (list of str): A list of query strings.</span>
<span class="sd">    - expected_chunks (list of str): A list of expected most similar document chunks corresponding to each query.</span>

<span class="sd">    Returns:</span>
<span class="sd">    - float: The accuracy of the search results.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">store</span> <span class="o">=</span> <span class="n">embed_and_store_documents</span><span class="p">(</span><span class="n">expected_chunks</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">expected_chunks</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">queries</span><span class="p">)))</span>  <span class="c1"># Embedding and storing documents plus some additional to ensure uniqueness</span>

    <span class="k">for</span> <span class="n">query</span><span class="p">,</span> <span class="n">expected</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">expected_chunks</span><span class="p">):</span>
        <span class="n">query_embedding</span> <span class="o">=</span> <span class="n">generate_query_embedding</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">most_similar</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">find_most_similar</span><span class="p">(</span><span class="n">query_embedding</span><span class="p">)</span>
        <span class="c1"># Assuming the expected_chunks are the document embeddings stored in the same order</span>
        <span class="k">if</span> <span class="n">most_similar</span> <span class="ow">and</span> <span class="n">most_similar</span> <span class="o">==</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">expected</span><span class="p">)]:</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">queries</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">accuracy</span>

<span class="c1"># Assuming the embed_and_store_documents, generate_query_embedding, and SimpleVectorStore are implemented as described</span>
</code></pre></div>
</li>
</ol>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.c8d2eff1.min.js"></script>
      
    
  </body>
</html>