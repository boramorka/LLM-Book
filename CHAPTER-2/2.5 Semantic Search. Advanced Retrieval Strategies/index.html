
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../2.4%20The%20Power%20of%20Embeddings/">
      
      
        <link rel="next" href="../2.6%20RAG%20Systems.%20Techniques%20for%20Question%20Answering/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.17">
    
    
      
        <title>2.5 Semantic Search. Advanced Retrieval Strategies - LLMOps. Make AI Work For You.</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#25-semantic-search-advanced-retrieval-strategies" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="LLMOps. Make AI Work For You." class="md-header__button md-logo" aria-label="LLMOps. Make AI Work For You." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LLMOps. Make AI Work For You.
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2.5 Semantic Search. Advanced Retrieval Strategies
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/boramorka/LLM-book" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    boramorka/LLM-book
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="LLMOps. Make AI Work For You." class="md-nav__button md-logo" aria-label="LLMOps. Make AI Work For You." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    LLMOps. Make AI Work For You.
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/boramorka/LLM-book" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    boramorka/LLM-book
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    CHAPTER-1. OPEN AI API
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            CHAPTER-1. OPEN AI API
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/1.1%20Introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.1 Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/1.2%20Classification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.2 Classification
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/1.3%20Advanced%20Moderaton/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.3 Advanced Moderaton
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/1.4%20Elevating%20Machine%20Reasoning%3A%20Advanced%20Strategies/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.4 Elevating Machine Reasoning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/1.5%20The%20Power%20of%20Prompt%20Chaining/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.5 The Power of Prompt Chaining
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/1.6%20Building%20and%20Evaluating%20LLM%20Applications/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.6 Building and Evaluating LLM Applications
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/1.7%20Summary%20and%20Reflections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.7 Summary and Reflections
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    CHAPTER-2. LANGCHAIN
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            CHAPTER-2. LANGCHAIN
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2.1%20Introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2.1 Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2.2%20LangChain%20Document%20Loaders/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2.2 LangChain Document Loaders
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2.3%20Deep%20Dive%20into%20Text%20Splitting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2.3 Deep Dive into Text Splitting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2.4%20The%20Power%20of%20Embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2.4 The Power of Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    2.5 Semantic Search. Advanced Retrieval Strategies
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    2.5 Semantic Search. Advanced Retrieval Strategies
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#maximum-marginal-relevance-mmr" class="md-nav__link">
    <span class="md-ellipsis">
      Maximum Marginal Relevance (MMR)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#self-query-retrieval" class="md-nav__link">
    <span class="md-ellipsis">
      Self-Query Retrieval
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contextual-compression" class="md-nav__link">
    <span class="md-ellipsis">
      Contextual Compression
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2.6%20RAG%20Systems.%20Techniques%20for%20Question%20Answering/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2.6 RAG Systems. Techniques for Question Answering
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2.7%20Building%20Chatbots%20with%20LangChain/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2.7 Building Chatbots with LangChain
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2.8%20Summary%20and%20Reflections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2.8 Summary and Reflections
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    CHAPTER-3. LLMOPS
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            CHAPTER-3. LLMOPS
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-3/3.1%20Introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.1 Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-3/3.2%20Mastering%20LLM%20Workflows%20with%20Kubeflow%20Pipelines/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.2 Mastering LLM Workflows with Kubeflow Pipelines
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-3/3.3%20Implementing%20the%20AI%20Quiz%20Generation%20Mechanism/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.3 Implementing the AI Quiz Generation Mechanism
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-3/3.4%20Summary%20and%20Reflections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.4 Summary and Reflections
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Answers
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Answers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/Answers%201.1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.1 Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/Answers%201.2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.2 Classification
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/Answers%201.3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.3 Advanced Moderaton
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/Answers%201.4/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.4 Elevating Machine Reasoning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/Answers%201.5/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.5 The Power of Prompt Chaining
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-1/Answers%201.6/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.6 Building and Evaluating LLM Applications
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Answers%202.2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2.2 LangChain Document Loaders
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Answers%202.3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2.3 Deep Dive into Text Splitting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Answers%202.4/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2.4 The Power of Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Answers%202.5/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2.5 Semantic Search. Advanced Retrieval Strategies
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Answers%202.6/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2.6 RAG Systems. Techniques for Question Answering
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Answers%202.7/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2.7 Building Chatbots with LangChain
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-3/Answers%203.2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.2 Mastering LLM Workflows with Kubeflow Pipelines
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CHAPTER-3/Answers%203.3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.3 Implementing the AI Quiz Generation Mechanism
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="25-semantic-search-advanced-retrieval-strategies">2.5 Semantic Search. Advanced Retrieval Strategies</h1>
<h2 id="introduction">Introduction</h2>
<p>The ability to accurately retrieve relevant information from a large corpus of data is crucial in the development of intelligent systems, such as chatbots and question-answering models. While semantic search offers a solid foundation for such tasks, it often encounters edge cases where its effectiveness diminishes. This chapter delves into advanced retrieval methods designed to overcome these limitations, thereby improving the precision and diversity of the retrieved information.</p>
<p>Semantic search, by relying solely on semantic similarity, may not always yield the most informative or diverse set of results. Advanced retrieval methods address this by incorporating mechanisms to ensure that the information retrieved is not only relevant but also varied and comprehensive. Such techniques are essential for handling complex queries that require nuanced responses.</p>
<h2 id="maximum-marginal-relevance-mmr">Maximum Marginal Relevance (MMR)</h2>
<p>MMR is a technique designed to balance relevance and diversity in the set of retrieved documents. It operates by selecting documents that are not only semantically close to the query but also diverse among themselves. This approach is particularly useful in scenarios where providing a broad spectrum of information is crucial for adequately answering a query.</p>
<p>The process involves initially fetching a larger set of documents based on semantic similarity. From this set, documents are then selected based on their relevance to the query and their diversity compared to already selected documents. This method ensures that the final set of documents provides a well-rounded perspective on the query topic.</p>
<h2 id="self-query-retrieval">Self-Query Retrieval</h2>
<p>Self-query retrieval is adept at handling queries that contain both semantic and metadata components. For example, a query asking for movies about aliens made in 1980 combines a semantic element ("movies about aliens") with a metadata filter ("made in 1980"). This method splits the query into these two components, using semantic search for the former and metadata filtering for the latter.</p>
<h2 id="contextual-compression">Contextual Compression</h2>
<p>Contextual compression involves extracting the most relevant segments from retrieved documents. This technique is valuable when the entirety of a document is not necessary for answering a query, focusing instead on the most pertinent information.</p>
<p>This method typically requires additional processing, as each retrieved document must be analyzed to identify and extract the relevant portions. While this may increase computational costs, it significantly enhances the quality and specificity of the information provided in response to a query.</p>
<h1 id="advanced-document-retrieval-techniques-for-enhanced-semantic-search">Advanced Document Retrieval Techniques for Enhanced Semantic Search</h1>
<h2 id="introduction_1">Introduction</h2>
<p>The retrieval of relevant documents from a vast corpus is a critical step in the workflow of Retrieval Augmented Generation (RAG), especially for applications like chatbots and question-answering systems. This chapter explores advanced retrieval techniques that improve upon basic semantic search by addressing common edge cases and enhancing result diversity and specificity.</p>
<h2 id="setting-up-the-environment">Setting Up the Environment</h2>
<p>Before diving into the core functionalities, it is essential to set up our working environment. This involves loading necessary libraries and configuring access to external services, such as OpenAI's API for embeddings. Below is the step-by-step guide to accomplish this:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Import necessary libraries</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>

<span class="c1"># Append the root directory to sys.path to ensure relative imports work correctly</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;../..&#39;</span><span class="p">)</span>

<span class="c1"># Load environment variables from a .env file for secure API key management</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dotenv</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dotenv</span><span class="p">,</span> <span class="n">find_dotenv</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">load_dotenv</span><span class="p">(</span><span class="n">find_dotenv</span><span class="p">())</span>

<span class="c1"># Set the OpenAI API key from environment variables</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>

<span class="c1"># Ensure you have the necessary packages installed, including `lark` for parsing (if required)</span>
<span class="c1"># !pip install lark</span>
</code></pre></div>
<h2 id="initializing-the-vector-database-for-similarity-search">Initializing the Vector Database for Similarity Search</h2>
<p>Our objective is to create a vector database that can efficiently retrieve information based on semantic similarity. This involves embedding textual content into a high-dimensional vector space using OpenAI's embeddings. Here's how to initialize such a database:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Import the Chroma vector store and OpenAI embeddings from the langchain library</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.vectorstores</span><span class="w"> </span><span class="kn">import</span> <span class="n">Chroma</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>

<span class="c1"># Specify the directory where the vector database will persist its data</span>
<span class="n">persist_directory</span> <span class="o">=</span> <span class="s1">&#39;vector_db/chroma/&#39;</span>

<span class="c1"># Initialize the embedding function using OpenAI&#39;s model</span>
<span class="n">embedding_function</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">()</span>

<span class="c1"># Create a Chroma vector database instance with the specified persistence directory and embedding function</span>
<span class="n">vector_database</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">(</span>
    <span class="n">persist_directory</span><span class="o">=</span><span class="n">persist_directory</span><span class="p">,</span>
    <span class="n">embedding_function</span><span class="o">=</span><span class="n">embedding_function</span>
<span class="p">)</span>

<span class="c1"># Print the current number of entries in the vector database to verify it&#39;s ready for use</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vector_database</span><span class="o">.</span><span class="n">_collection</span><span class="o">.</span><span class="n">count</span><span class="p">())</span>
</code></pre></div>
<h2 id="populating-the-vector-database">Populating the Vector Database</h2>
<p>Next, we populate our vector database with a small set of textual data to demonstrate similarity search capabilities:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Define a list of texts to populate the database</span>
<span class="n">texts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;The Death Cap mushroom has a notable large fruiting body, often found above ground.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Among mushrooms, the Death Cap stands out for its large fruiting body, sometimes appearing in all-white.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The Death Cap, known for its toxicity, is one of the most dangerous mushrooms.&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="c1"># Create a smaller vector database from the given texts for demonstration purposes</span>
<span class="n">demo_vector_database</span> <span class="o">=</span> <span class="n">Chroma</span><span class="o">.</span><span class="n">from_texts</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">embedding_function</span><span class="o">=</span><span class="n">embedding_function</span><span class="p">)</span>

<span class="c1"># Define a query to search within the vector database</span>
<span class="n">query_text</span> <span class="o">=</span> <span class="s2">&quot;Discuss mushrooms characterized by their significant white fruiting bodies&quot;</span>

<span class="c1"># Perform a similarity search for the query, retrieving the top 2 most relevant entries</span>
<span class="n">similar_texts</span> <span class="o">=</span> <span class="n">demo_vector_database</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span><span class="n">query_text</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similarity Search Results:&quot;</span><span class="p">,</span> <span class="n">similar_texts</span><span class="p">)</span>

<span class="c1"># Perform a max marginal relevance search to find diverse yet relevant answers, fetching additional candidates for comparison</span>
<span class="n">diverse_texts</span> <span class="o">=</span> <span class="n">demo_vector_database</span><span class="o">.</span><span class="n">max_marginal_relevance_search</span><span class="p">(</span><span class="n">query_text</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">fetch_k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Diverse Search Results:&quot;</span><span class="p">,</span> <span class="n">diverse_texts</span><span class="p">)</span>
</code></pre></div>
<h2 id="advanced-retrieval-techniques">Advanced Retrieval Techniques</h2>
<h2 id="addressing-diversity-with-maximum-marginal-relevance-mmr">Addressing Diversity with Maximum Marginal Relevance (MMR)</h2>
<p>One common challenge in retrieval systems is ensuring that the search results are not only relevant but also diverse. This prevents the dominance of repetitive information and provides a broader perspective on the query subject. The Maximum Marginal Relevance (MMR) algorithm addresses this by balancing relevance to the query with diversity among the results.</p>
<h3 id="practical-implementation-of-mmr">Practical Implementation of MMR</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Define a query that seeks information</span>
<span class="n">query_for_information</span> <span class="o">=</span> <span class="s2">&quot;what insights are available on data analysis tools?&quot;</span>

<span class="c1"># Perform a standard similarity search to find the top 3 relevant documents</span>
<span class="n">top_similar_documents</span> <span class="o">=</span> <span class="n">vector_database</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span><span class="n">query_for_information</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Display the beginning of the content from the top two documents for comparison</span>
<span class="nb">print</span><span class="p">(</span><span class="n">top_similar_documents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">top_similar_documents</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>

<span class="c1"># Note the potential overlap in information. To introduce diversity, we apply MMR.</span>
<span class="n">diverse_documents</span> <span class="o">=</span> <span class="n">vector_database</span><span class="o">.</span><span class="n">max_marginal_relevance_search</span><span class="p">(</span><span class="n">query_for_information</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Display the beginning of the content from the top two diverse documents to observe the difference</span>
<span class="nb">print</span><span class="p">(</span><span class="n">diverse_documents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">diverse_documents</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
</code></pre></div>
<p>This code snippet illustrates the contrast between standard similarity search results and those obtained using MMR. By employing MMR, we ensure that the retrieved documents are not only relevant but also provide varied perspectives on the query.</p>
<h2 id="enhancing-specificity-using-metadata">Enhancing Specificity Using Metadata</h2>
<p>Vector databases often contain rich metadata that can be exploited to refine search queries further. Metadata provides additional context, allowing for more targeted searches that can filter results based on specific criteria.</p>
<h3 id="leveraging-metadata-for-targeted-searches">Leveraging Metadata for Targeted Searches</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Define a query with a specific context in mind</span>
<span class="n">specific_query</span> <span class="o">=</span> <span class="s2">&quot;what discussions were there about regression analysis in the third lecture?&quot;</span>

<span class="c1"># Execute a similarity search with a metadata filter to target documents from a specific lecture</span>
<span class="n">targeted_documents</span> <span class="o">=</span> <span class="n">vector_database</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span>
    <span class="n">specific_query</span><span class="p">,</span>
    <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="nb">filter</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;documents/cs229_lectures/MachineLearning-Lecture03.pdf&quot;</span><span class="p">}</span>
<span class="p">)</span>

<span class="c1"># Iterate through the results to display their metadata, highlighting the specificity of the search</span>
<span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">targeted_documents</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">document</span><span class="o">.</span><span class="n">metadata</span><span class="p">)</span>
</code></pre></div>
<h2 id="using-metadata-with-self-query-retrievers">Using Metadata with Self-Query Retrievers</h2>
<p>Metadata serves as contextual information that can significantly refine search results. When combined with the capabilities of a self-query retriever, it becomes possible to automatically extract both the query string and the relevant metadata filters from a single input query. This approach eliminates the need for manual metadata specification, making the search process both efficient and intuitive.</p>
<h3 id="initializing-the-environment-and-defining-metadata">Initializing the Environment and Defining Metadata</h3>
<p>Before we can execute a metadata-aware search, we need to set up our environment and define the metadata attributes we intend to use:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Import necessary modules from the langchain library</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.retrievers.self_query.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">SelfQueryRetriever</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.chains.query_constructor.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">AttributeInfo</span>

<span class="c1"># Define metadata attributes with detailed descriptions</span>
<span class="n">metadata_attributes</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">AttributeInfo</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;source&quot;</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Specifies the lecture document, limited to specific files within the `docs/cs229_lectures` directory.&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">AttributeInfo</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;page&quot;</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;The page number within the lecture document.&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;integer&quot;</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">]</span>

<span class="c1"># Note: Transition to using OpenAI&#39;s gpt-3.5-turbo-instruct model due to deprecation of the previous default model.</span>
<span class="n">document_content_description</span> <span class="o">=</span> <span class="s2">&quot;Detailed lecture notes&quot;</span>
<span class="n">language_model</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s1">&#39;gpt-4o-mini&#39;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<h3 id="configuring-the-self-query-retriever">Configuring the Self-Query Retriever</h3>
<p>The next step involves configuring the self-query retriever with our language model, vector database, and the defined metadata attributes:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Initialize the self-query retriever with the language model, vector database, and metadata attributes</span>
<span class="n">self_query_retriever</span> <span class="o">=</span> <span class="n">SelfQueryRetriever</span><span class="o">.</span><span class="n">from_llm</span><span class="p">(</span>
    <span class="n">language_model</span><span class="p">,</span>
    <span class="n">vector_database</span><span class="p">,</span>
    <span class="n">document_content_description</span><span class="p">,</span>
    <span class="n">metadata_attributes</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</code></pre></div>
<h3 id="executing-a-query-with-automatic-metadata-inference">Executing a Query with Automatic Metadata Inference</h3>
<p>Now, let's perform a search that automatically infers relevant metadata from the query itself:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Define a query that specifies the context within the question</span>
<span class="n">specific_query</span> <span class="o">=</span> <span class="s2">&quot;what insights are provided on regression analysis in the third lecture?&quot;</span>

<span class="c1"># Note: The first execution may trigger a deprecation warning for `predict_and_parse`, which can be ignored.</span>
<span class="c1"># Retrieve documents relevant to the specific query, leveraging inferred metadata for precision</span>
<span class="n">relevant_documents</span> <span class="o">=</span> <span class="n">self_query_retriever</span><span class="o">.</span><span class="n">get_relevant_documents</span><span class="p">(</span><span class="n">specific_query</span><span class="p">)</span>

<span class="c1"># Display the metadata of retrieved documents to demonstrate the specificity of the search</span>
<span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">relevant_documents</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">document</span><span class="o">.</span><span class="n">metadata</span><span class="p">)</span>
</code></pre></div>
<h2 id="implementing-contextual-compression">Implementing Contextual Compression</h2>
<p>Contextual compression works by extracting segments of a document that are most relevant to a given query. This method not only reduces the computational load on LLMs but also enhances the quality of the responses by focusing on the most pertinent information.</p>
<h3 id="setting-up-the-environment_1">Setting Up the Environment</h3>
<p>Before diving into the specifics of contextual compression, ensure that your environment is properly configured with the necessary libraries:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Import the necessary classes for contextual compression and document retrieval</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.retrievers</span><span class="w"> </span><span class="kn">import</span> <span class="n">ContextualCompressionRetriever</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.retrievers.document_compressors</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLMChainExtractor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
</code></pre></div>
<h3 id="initializing-the-compression-tools">Initializing the Compression Tools</h3>
<p>The next step involves initializing the compression mechanism with a pre-trained language model, which will be used to identify and extract the relevant portions of documents:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Initialize the language model with a specific configuration to ensure deterministic behavior</span>
<span class="n">language_model</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">)</span>

<span class="c1"># Create a compressor using the language model for extracting relevant text segments</span>
<span class="n">document_compressor</span> <span class="o">=</span> <span class="n">LLMChainExtractor</span><span class="o">.</span><span class="n">from_llm</span><span class="p">(</span><span class="n">language_model</span><span class="p">)</span>
</code></pre></div>
<h3 id="creating-the-contextual-compression-retriever">Creating the Contextual Compression Retriever</h3>
<p>With the compressor ready, we can now set up a retriever that integrates contextual compression into the retrieval process:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Combine the document compressor with the existing vector database retriever</span>
<span class="n">compression_retriever</span> <span class="o">=</span> <span class="n">ContextualCompressionRetriever</span><span class="p">(</span>
    <span class="n">base_compressor</span><span class="o">=</span><span class="n">document_compressor</span><span class="p">,</span>
    <span class="n">base_retriever</span><span class="o">=</span><span class="n">vector_database</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">()</span>
<span class="p">)</span>
</code></pre></div>
<h3 id="retrieving-compressed-documents">Retrieving Compressed Documents</h3>
<p>Let's execute a query and observe how the contextual compression retriever returns a more focused set of documents:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Define a query for which we seek relevant document segments</span>
<span class="n">query_text</span> <span class="o">=</span> <span class="s2">&quot;what insights are offered on data analysis tools?&quot;</span>

<span class="c1"># Retrieve documents relevant to the query, automatically compressed for relevance</span>
<span class="n">compressed_documents</span> <span class="o">=</span> <span class="n">compression_retriever</span><span class="o">.</span><span class="n">get_relevant_documents</span><span class="p">(</span><span class="n">query_text</span><span class="p">)</span>

<span class="c1"># Function to nicely format and print the content of compressed documents</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pretty_print_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;-&#39;</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;Document </span><span class="si">{</span><span class="n">index</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">:</span><span class="se">\n\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">doc</span><span class="o">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">documents</span><span class="p">)]))</span>

<span class="c1"># Display the compressed documents</span>
<span class="n">pretty_print_documents</span><span class="p">(</span><span class="n">compressed_documents</span><span class="p">)</span>
</code></pre></div>
<h2 id="implementing-contextual-compression-with-mmr-for-document-retrieval">Implementing Contextual Compression with MMR for Document Retrieval</h2>
<p>Contextual compression aims to distill documents to their essence by focusing on segments most relevant to a query. When paired with the MMR strategy, it balances relevance with diversity in the retrieved documents, ensuring a broader perspective on the queried topic.</p>
<h3 id="setting-up-the-compression-based-retriever-with-mmr">Setting Up the Compression-Based Retriever with MMR</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Initialize the contextual compression retriever with MMR for diverse and relevant document retrieval</span>
<span class="n">compression_based_retriever</span> <span class="o">=</span> <span class="n">ContextualCompressionRetriever</span><span class="p">(</span>
    <span class="n">base_compressor</span><span class="o">=</span><span class="n">document_compressor</span><span class="p">,</span>
    <span class="n">base_retriever</span><span class="o">=</span><span class="n">vector_database</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(</span><span class="n">search_type</span><span class="o">=</span><span class="s2">&quot;mmr&quot;</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Define a query to test the combined approach</span>
<span class="n">query_for_insights</span> <span class="o">=</span> <span class="s2">&quot;what insights are available on statistical analysis methods?&quot;</span>

<span class="c1"># Retrieve compressed documents using the contextual compression retriever</span>
<span class="n">compressed_documents</span> <span class="o">=</span> <span class="n">compression_based_retriever</span><span class="o">.</span><span class="n">get_relevant_documents</span><span class="p">(</span><span class="n">query_for_insights</span><span class="p">)</span>

<span class="c1"># Utilize a helper function to print the contents of the retrieved, compressed documents</span>
<span class="n">pretty_print_documents</span><span class="p">(</span><span class="n">compressed_documents</span><span class="p">)</span>
</code></pre></div>
<p>This approach optimizes document retrieval by ensuring that the results are not only relevant but also diverse, preventing redundancy and enhancing the user's understanding of the subject matter.</p>
<h2 id="exploring-alternative-document-retrieval-methods">Exploring Alternative Document Retrieval Methods</h2>
<p>Beyond the vector-based retrieval methods, the LangChain library supports a variety of other document retrieval strategies, such as TF-IDF and SVM. These methods offer different advantages based on the specific requirements of the application.</p>
<h3 id="loading-and-preparing-documents">Loading and Preparing Documents</h3>
<p>Before implementing alternative retrieval strategies, it's crucial to prepare the documents by loading and splitting the text appropriately.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Load a document using the PyPDFLoader</span>
<span class="n">document_loader</span> <span class="o">=</span> <span class="n">PyPDFLoader</span><span class="p">(</span><span class="s2">&quot;docs/cs229_lectures/MachineLearning-Lecture01.pdf&quot;</span><span class="p">)</span>
<span class="n">document_pages</span> <span class="o">=</span> <span class="n">document_loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1"># Concatenate all page texts into a single string for processing</span>
<span class="n">complete_document_text</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">page</span><span class="o">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">page</span> <span class="ow">in</span> <span class="n">document_pages</span><span class="p">])</span>

<span class="c1"># Split the complete document text into manageable chunks using a text splitter</span>
<span class="n">text_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">1500</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
<span class="n">text_chunks</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_text</span><span class="p">(</span><span class="n">complete_document_text</span><span class="p">)</span>
</code></pre></div>
<h3 id="implementing-tf-idf-and-svm-retrievers">Implementing TF-IDF and SVM Retrievers</h3>
<p>With the document text prepared, we can now utilize TF-IDF and SVM-based retrievers for document retrieval.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Initialize a SVM-based retriever from the text chunks</span>
<span class="n">svm_based_retriever</span> <span class="o">=</span> <span class="n">SVMRetriever</span><span class="o">.</span><span class="n">from_texts</span><span class="p">(</span><span class="n">text_chunks</span><span class="p">,</span> <span class="n">embedding_function</span><span class="p">)</span>

<span class="c1"># Similarly, initialize a TF-IDF-based retriever from the same text chunks</span>
<span class="n">tfidf_based_retriever</span> <span class="o">=</span> <span class="n">TFIDFRetriever</span><span class="o">.</span><span class="n">from_texts</span><span class="p">(</span><span class="n">text_chunks</span><span class="p">)</span>

<span class="c1"># Perform document retrieval using the SVM retriever for a specific query</span>
<span class="n">query_on_major_topics</span> <span class="o">=</span> <span class="s2">&quot;What are major topics for this class?&quot;</span>
<span class="n">svm_retrieval_results</span> <span class="o">=</span> <span class="n">svm_based_retriever</span><span class="o">.</span><span class="n">get_relevant_documents</span><span class="p">(</span><span class="n">query_on_major_topics</span><span class="p">)</span>

<span class="c1"># Perform another retrieval using the TF-IDF retriever for a different query</span>
<span class="n">query_on_specific_tool</span> <span class="o">=</span> <span class="s2">&quot;what did they say about statistical software?&quot;</span>
<span class="n">tfidf_retrieval_results</span> <span class="o">=</span> <span class="n">tfidf_based_retriever</span><span class="o">.</span><span class="n">get_relevant_documents</span><span class="p">(</span><span class="n">query_on_specific_tool</span><span class="p">)</span>

<span class="c1"># Print the first document from the retrieval results as an example</span>
<span class="nb">print</span><span class="p">(</span><span class="n">svm_retrieval_results</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tfidf_retrieval_results</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>
<h2 id="best-practices">Best Practices</h2>
<ol>
<li>
<p><strong>Balanced Use of MMR</strong>: When utilizing Maximum Marginal Relevance (MMR), it's crucial to find a balance between relevance and diversity. This ensures that the retrieved documents provide a comprehensive view of the query topic without sacrificing pertinence.</p>
</li>
<li>
<p><strong>Effective Metadata Utilization</strong>: Metadata can significantly enhance the specificity of search results. Designing and implementing a well-thought-out metadata schema allows for more targeted searches, especially when combined with self-query retrieval techniques.</p>
</li>
<li>
<p><strong>Optimization of Contextual Compression</strong>: While contextual compression provides a focused subset of information, it requires additional processing. It's important to optimize this step to balance computational costs with the benefits of increased specificity and relevance.</p>
</li>
<li>
<p><strong>Strategic Document Preparation</strong>: For alternative retrieval methods like TF-IDF and SVM, the way documents are prepared and processed (e.g., text chunking) can greatly affect the outcome. Tailoring these processes to your specific use case can lead to more efficient and accurate retrievals.</p>
</li>
<li>
<p><strong>Model and Method Selection</strong>: The choice of language models and retrieval techniques should be informed by the nature of your data and the specific needs of your application. Regularly review and update these choices as newer models and methods become available.</p>
</li>
</ol>
<h2 id="conclusion">Conclusion</h2>
<p>This chapter has explored various advanced retrieval techniques designed to enhance the performance of semantic search systems. By addressing limitations related to diversity, specificity, and information relevance, these methods offer a pathway to more intelligent and efficient retrieval systems. Through the practical application of MMR, self-query retrieval, contextual compression, and alternative document retrieval methods, developers can build systems that not only understand the semantic content of queries but also provide rich, diverse, and targeted responses.</p>
<p>Adhering to best practices in the implementation of these techniques ensures that retrieval systems are both effective and efficient. As the field of NLP continues to evolve, staying informed about the latest advancements in retrieval technologies will be key to maintaining the edge in semantic search capabilities. </p>
<p>In summary, the integration of advanced retrieval techniques into semantic search systems represents a significant step forward in the development of intelligent information retrieval systems. By carefully selecting and optimizing these methods, developers can create solutions that significantly improve the user experience, delivering precise, diverse, and contextually relevant information in response to complex queries.</p>
<h2 id="theory-questions">Theory questions:</h2>
<ol>
<li>Describe the principle of Maximum Marginal Relevance (MMR) and its role in improving information retrieval.</li>
<li>How does self-query retrieval address the challenge of queries that combine semantic and metadata components?</li>
<li>Explain the concept of contextual compression in document retrieval and its significance.</li>
<li>Detail the steps involved in setting up an environment for advanced retrieval techniques using OpenAI's API and the langchain library.</li>
<li>How does the initialization of a vector database contribute to efficient semantic similarity search?</li>
<li>Describe the process of populating and utilizing a vector database for similarity and diverse search purposes.</li>
<li>In the context of advanced document retrieval, what are the advantages of using MMR to address diversity in search results?</li>
<li>How can metadata be leveraged to enhance the specificity of search results in document retrieval systems?</li>
<li>Discuss the advantages and implementation challenges of self-query retrievers in semantic search.</li>
<li>Explain the role of contextual compression in reducing computational load and improving response quality in retrieval systems.</li>
<li>What are the key best practices for implementing advanced retrieval techniques in semantic search systems?</li>
<li>Compare and contrast the effectiveness of vector-based retrieval methods with alternative strategies like TF-IDF and SVM in document retrieval.</li>
<li>How does the integration of advanced retrieval techniques improve the performance and user experience of semantic search systems?</li>
<li>Discuss the potential impact of evolving NLP technologies on the future development of advanced retrieval techniques for semantic search.</li>
</ol>
<h2 id="practice-questions">Practice questions:</h2>
<ol>
<li>Implement a Python class <code>VectorDatabase</code> with the following methods:</li>
<li><code>__init__(self, persist_directory: str)</code>: Constructor that initializes the vector database with a persistence directory.</li>
<li><code>add_text(self, text: str)</code>: Embeds the given text into a high-dimensional vector using OpenAI's embeddings and stores it in the database. Assume you have access to a function <code>openai_embedding(text: str) -&gt; List[float]</code> that returns the embedding vector.</li>
<li>
<p><code>similarity_search(self, query: str, k: int) -&gt; List[str]</code>: Performs a similarity search for the query, returning the top <code>k</code> most similar texts from the database. Use a placeholder similarity function for the implementation.</p>
</li>
<li>
<p>Create a function <code>compress_document</code> that takes a list of strings (document) and a query string as input and returns a list of strings, where each string is a compressed segment of the document relevant to the query. Assume there's an external utility function <code>compress_segment(segment: str, query: str) -&gt; str</code> that compresses a single document segment based on the query.</p>
</li>
<li>
<p>Develop a function <code>max_marginal_relevance</code> that takes a list of document IDs, a query, and two parameters <code>lambda</code> and <code>k</code>, then returns a list of <code>k</code> document IDs selected based on Maximum Marginal Relevance (MMR). Assume you have a similarity function <code>similarity(doc_id: str, query: str) -&gt; float</code> that measures the similarity between a document and the query, and a diversity function <code>diversity(doc_id1: str, doc_id2: str) -&gt; float</code> that measures the diversity between two documents.</p>
</li>
<li>
<p>Write a function <code>initialize_vector_db</code> that demonstrates how to populate a vector database with a list of predefined texts and then perform a similarity search and a diverse search. The function should print out the results of both searches. Use the <code>VectorDatabase</code> class you implemented in task 2 for the vector database.</p>
</li>
</ol>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
    
  </body>
</html>