
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.17">
    
    
      
        <title>2.2 LangChain Document Loaders - LLMOps. Make AI Work For You.</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#22-langchain-document-loaders" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="LLMOps. Make AI Work For You." class="md-header__button md-logo" aria-label="LLMOps. Make AI Work For You." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LLMOps. Make AI Work For You.
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2.2 LangChain Document Loaders
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="Select language">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="../../en/" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="../../ru/" hreflang="ru" class="md-select__link">
              Русский
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/boramorka/LLM-book" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    boramorka/LLM-book
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="LLMOps. Make AI Work For You." class="md-nav__button md-logo" aria-label="LLMOps. Make AI Work For You." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    LLMOps. Make AI Work For You.
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/boramorka/LLM-book" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    boramorka/LLM-book
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="22-langchain-document-loaders">2.2 LangChain Document Loaders</h1>
<p>In the realm of data-driven applications, particularly those involving conversational interfaces and Large Language Models (LLMs), the ability to efficiently load, process, and interact with data from various sources is crucial. LangChain, an open-source framework, plays a pivotal role in this process with its extensive suite of document loaders designed to handle a wide range of data types and sources.</p>
<h2 id="understanding-document-loaders">Understanding Document Loaders</h2>
<p>Document loaders are specialized components of LangChain that facilitate the access and conversion of data from diverse formats and sources into a standardized document object. This object typically comprises content and associated metadata, enabling seamless integration and processing within LangChain applications. The versatility of document loaders supports data ingestion from websites, databases, and multimedia sources, handling formats such as PDFs, HTML, and JSON, among others.</p>
<p>LangChain offers over 80 different document loaders, each tailored to specific data sources and formats. This guide will focus on several critical types, providing a foundation for understanding and utilizing this powerful toolset.</p>
<p><strong>Unstructured Data Loaders</strong></p>
<p>These loaders are adept at handling data from public sources like YouTube, Twitter, and Hacker News, as well as proprietary sources such as Figma and Notion. They are essential for applications requiring access to a broad spectrum of unstructured data.</p>
<p><strong>Structured Data Loaders</strong></p>
<p>For applications involving tabular data with text cells or rows, structured data loaders come into play. They support sources like Airbyte, Stripe, and Airtable, enabling users to perform semantic search and question-answering over structured datasets.</p>
<h2 id="practical-guide-to-using-document-loaders">Practical Guide to Using Document Loaders</h2>
<h3 id="setup-and-configuration">Setup and Configuration</h3>
<p>Before interacting with external data, it's important to configure the environment correctly. This includes installing necessary packages and setting up API keys for services like OpenAI. </p>
<div class="highlight"><pre><span></span><code><span class="c1"># Install necessary packages (Note: These may already be installed in your environment)</span>
<span class="c1"># !pip install langchain dotenv</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dotenv</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dotenv</span><span class="p">,</span> <span class="n">find_dotenv</span>

<span class="c1"># Load environment variables from a .env file</span>
<span class="n">load_dotenv</span><span class="p">(</span><span class="n">find_dotenv</span><span class="p">())</span>

<span class="c1"># Set the OpenAI API key from the environment variables</span>
<span class="n">openai_api_key</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;OPENAI_API_KEY&#39;</span><span class="p">]</span>
</code></pre></div>
<h3 id="loading-pdf-documents">Loading PDF Documents</h3>
<p>One common source of data is PDF documents. The following example demonstrates how to load a PDF document, specifically a transcript from a lecture seriess.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.document_loaders</span><span class="w"> </span><span class="kn">import</span> <span class="n">PyPDFLoader</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">Counter</span>

<span class="c1"># Initialize the PDF Loader with the path to the PDF document</span>
<span class="n">pdf_loader</span> <span class="o">=</span> <span class="n">PyPDFLoader</span><span class="p">(</span><span class="s2">&quot;docs/lecture_series/Lecture01.pdf&quot;</span><span class="p">)</span>

<span class="c1"># Load the document pages</span>
<span class="n">document_pages</span> <span class="o">=</span> <span class="n">pdf_loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1"># Function to clean and tokenize text</span>
<span class="k">def</span><span class="w"> </span><span class="nf">clean_and_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># Remove non-alphabetic characters and split text into words</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\b[a-z]+\b&#39;</span><span class="p">,</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">words</span>

<span class="c1"># Initialize a Counter object to keep track of word frequencies</span>
<span class="n">word_frequencies</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>

<span class="c1"># Iterate over each page in the document</span>
<span class="k">for</span> <span class="n">page</span> <span class="ow">in</span> <span class="n">document_pages</span><span class="p">:</span>
    <span class="c1"># Check if the page is not blank</span>
    <span class="k">if</span> <span class="n">page</span><span class="o">.</span><span class="n">page_content</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>
        <span class="c1"># Clean and tokenize the page content</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">clean_and_tokenize</span><span class="p">(</span><span class="n">page</span><span class="o">.</span><span class="n">page_content</span><span class="p">)</span>
        <span class="c1"># Update word frequencies</span>
        <span class="n">word_frequencies</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Handle blank page</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Blank page found at index </span><span class="si">{</span><span class="n">document_pages</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">page</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Example: Print the 10 most common words in the document</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Most common words in the document:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_frequencies</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">freq</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Accessing metadata of the first page as an example</span>
<span class="n">first_page_metadata</span> <span class="o">=</span> <span class="n">document_pages</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Metadata of the first page:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">first_page_metadata</span><span class="p">)</span>

<span class="c1"># Optional: Save the clean text of the document to a file</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;cleaned_lecture_series_lecture01.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">text_file</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">page</span> <span class="ow">in</span> <span class="n">document_pages</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">page</span><span class="o">.</span><span class="n">page_content</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>  <span class="c1"># Check if the page is not blank</span>
            <span class="n">cleaned_text</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">clean_and_tokenize</span><span class="p">(</span><span class="n">page</span><span class="o">.</span><span class="n">page_content</span><span class="p">))</span>
            <span class="n">text_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">cleaned_text</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<p>This example includes the following additional steps:</p>
<ol>
<li>
<p><strong>Text Cleaning and Tokenization</strong>: A function <code>clean_and_tokenize</code> is added to remove any non-alphabetic characters and split the text into lowercase words for basic normalization.</p>
</li>
<li>
<p><strong>Word Frequency Analysis</strong>: Using the <code>Counter</code> class from the <code>collections</code> module, the script now counts the frequency of each word across the entire document. This can be useful for understanding the most discussed topics or keywords in the lecture series.</p>
</li>
<li>
<p><strong>Handling Blank Pages</strong>: It checks for blank pages and prints a message if any are found. This is helpful for debugging issues with document loading or to ensure that all content is being accurately captured.</p>
</li>
<li>
<p><strong>Saving Cleaned Text</strong>: Optionally, the script can save the cleaned and tokenized text of the document to a file. This could be useful for further analysis or processing, such as feeding the text into a natural language processing pipeline.</p>
</li>
</ol>
<p>This expanded code provides a more comprehensive example of processing PDF documents programmatically, from loading and cleaning the text to basic analysis and handling special cases.</p>
<h3 id="transcribing-youtube-videos">Transcribing YouTube Videos</h3>
<p>Another valuable data source is YouTube videos. The following code block demonstrates how to load audio from a YouTube video, transcribe it using OpenAI's Whisper model, and access the transcription.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.document_loaders.generic</span><span class="w"> </span><span class="kn">import</span> <span class="n">GenericLoader</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.document_loaders.parsers</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAIWhisperParser</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.document_loaders.blob_loaders.youtube_audio</span><span class="w"> </span><span class="kn">import</span> <span class="n">YoutubeAudioLoader</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.tokenize</span><span class="w"> </span><span class="kn">import</span> <span class="n">sent_tokenize</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">textblob</span><span class="w"> </span><span class="kn">import</span> <span class="n">TextBlob</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>

<span class="c1"># Make sure nltk resources are downloaded (e.g., punkt for sentence tokenization)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>

<span class="c1"># Specify the YouTube video URL and the directory to save the audio files</span>
<span class="n">video_url</span> <span class="o">=</span> <span class="s2">&quot;https://www.youtube.com/watch?v=example_video_id&quot;</span>
<span class="n">audio_save_directory</span> <span class="o">=</span> <span class="s2">&quot;docs/youtube/&quot;</span>

<span class="c1"># Ensure the directory exists</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">audio_save_directory</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Initialize the Generic Loader with the YouTube Audio Loader and Whisper Parser</span>
<span class="n">youtube_loader</span> <span class="o">=</span> <span class="n">GenericLoader</span><span class="p">(</span>
    <span class="n">YoutubeAudioLoader</span><span class="p">([</span><span class="n">video_url</span><span class="p">],</span> <span class="n">audio_save_directory</span><span class="p">),</span>
    <span class="n">OpenAIWhisperParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="c1"># Load the document</span>
<span class="n">youtube_documents</span> <span class="o">=</span> <span class="n">youtube_loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1"># Example: Accessing the first part of the transcribed content</span>
<span class="n">transcribed_text</span> <span class="o">=</span> <span class="n">youtube_documents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="p">[:</span><span class="mi">500</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">transcribed_text</span><span class="p">)</span>

<span class="c1"># Break down the transcription into sentences</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">transcribed_text</span><span class="p">)</span>

<span class="c1"># Print the first 5 sentences as an example</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">First 5 sentences of the transcription:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

<span class="c1"># Perform sentiment analysis on the transcribed content</span>
<span class="n">sentiment</span> <span class="o">=</span> <span class="n">TextBlob</span><span class="p">(</span><span class="n">transcribed_text</span><span class="p">)</span><span class="o">.</span><span class="n">sentiment</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Sentiment Analysis:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Polarity: </span><span class="si">{</span><span class="n">sentiment</span><span class="o">.</span><span class="n">polarity</span><span class="si">}</span><span class="s2">, Subjectivity: </span><span class="si">{</span><span class="n">sentiment</span><span class="o">.</span><span class="n">subjectivity</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Polarity is a float within the range [-1.0, 1.0], where -1 means negative sentiment and 1 means positive sentiment.</span>
<span class="c1"># Subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.</span>

<span class="c1"># Additional analysis or processing can be done here, such as:</span>
<span class="c1"># - Extracting named entities (names, places, etc.)</span>
<span class="c1"># - Identifying key phrases or topics</span>
<span class="c1"># - Summarizing the content</span>
</code></pre></div>
<p>This example demonstrates additional steps that can be performed after transcribing YouTube video content:</p>
<ol>
<li>
<p><strong>Sentence Tokenization</strong>: Breaking down the transcribed text into individual sentences, which could be useful for further detailed analysis or processing.</p>
</li>
<li>
<p><strong>Sentiment Analysis</strong>: Using <code>TextBlob</code> to perform basic sentiment analysis on the transcribed text. This gives an idea of the overall tone of the video - whether it's more positive, negative, or neutral, and how subjective (opinionated) or objective (factual) the content is.</p>
</li>
<li>
<p><strong>Placeholder for Further Analysis</strong>: Suggestions for additional analysis include extracting named entities, identifying key phrases or topics, and summarizing the content. These steps would require more sophisticated NLP tools and libraries, which can be integrated based on the specific requirements of the project.</p>
</li>
</ol>
<p>This code offers a foundation for not only transcribing YouTube videos but also for beginning to understand and analyze the content of those transcriptions in a structured and automated way.</p>
<h3 id="loading-content-from-urls">Loading Content from URLs</h3>
<p>Web content is an inexhaustible source of data. The following example showcases how to load content from a specific URL, such as an educational article or a company handbook.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.document_loaders</span><span class="w"> </span><span class="kn">import</span> <span class="n">WebBaseLoader</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">bs4</span><span class="w"> </span><span class="kn">import</span> <span class="n">BeautifulSoup</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.tokenize</span><span class="w"> </span><span class="kn">import</span> <span class="n">sent_tokenize</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.corpus</span><span class="w"> </span><span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.probability</span><span class="w"> </span><span class="kn">import</span> <span class="n">FreqDist</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk</span><span class="w"> </span><span class="kn">import</span> <span class="n">download</span>
<span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>
<span class="n">download</span><span class="p">(</span><span class="s1">&#39;stopwords&#39;</span><span class="p">)</span>

<span class="c1"># Initialize the Web Base Loader with the target URL</span>
<span class="n">web_loader</span> <span class="o">=</span> <span class="n">WebBaseLoader</span><span class="p">(</span><span class="s2">&quot;https://example.com/path/to/document&quot;</span><span class="p">)</span>

<span class="c1"># Load the document</span>
<span class="n">web_documents</span> <span class="o">=</span> <span class="n">web_loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1"># Use BeautifulSoup to parse the HTML content</span>
<span class="n">soup</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">web_documents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="p">,</span> <span class="s1">&#39;html.parser&#39;</span><span class="p">)</span>

<span class="c1"># Example: Cleaning the web content by removing script and style elements</span>
<span class="k">for</span> <span class="n">script_or_style</span> <span class="ow">in</span> <span class="n">soup</span><span class="p">([</span><span class="s2">&quot;script&quot;</span><span class="p">,</span> <span class="s2">&quot;style&quot;</span><span class="p">]):</span>
    <span class="n">script_or_style</span><span class="o">.</span><span class="n">decompose</span><span class="p">()</span>

<span class="c1"># Get text from the HTML page and replace multiple spaces/newlines with single space</span>
<span class="n">clean_text</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">soup</span><span class="o">.</span><span class="n">stripped_strings</span><span class="p">)</span>

<span class="c1"># Print the first 500 characters of the cleaned web content</span>
<span class="nb">print</span><span class="p">(</span><span class="n">clean_text</span><span class="p">[:</span><span class="mi">500</span><span class="p">])</span>

<span class="c1"># Extracting specific information</span>
<span class="c1"># Example: Extracting all hyperlinks</span>
<span class="n">links</span> <span class="o">=</span> <span class="p">[(</span><span class="n">a</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">a</span><span class="p">[</span><span class="s1">&#39;href&#39;</span><span class="p">])</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">soup</span><span class="o">.</span><span class="n">find_all</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="n">href</span><span class="o">=</span><span class="kc">True</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Extracted links:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">text</span><span class="p">,</span> <span class="n">href</span> <span class="ow">in</span> <span class="n">links</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>  <span class="c1"># Print first 5 links as an example</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">href</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Example: Extracting headings (h1)</span>
<span class="n">headings</span> <span class="o">=</span> <span class="p">[</span><span class="n">h1</span><span class="o">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">h1</span> <span class="ow">in</span> <span class="n">soup</span><span class="o">.</span><span class="n">find_all</span><span class="p">(</span><span class="s1">&#39;h1&#39;</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Headings found on the page:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">heading</span> <span class="ow">in</span> <span class="n">headings</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">heading</span><span class="p">)</span>

<span class="c1"># Text summarization</span>
<span class="c1"># Tokenize sentences</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">clean_text</span><span class="p">)</span>
<span class="c1"># Filter out stopwords</span>
<span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">))</span>
<span class="n">filtered_sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">if</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">])</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>

<span class="c1"># Frequency distribution of words</span>
<span class="n">word_freq</span> <span class="o">=</span> <span class="n">FreqDist</span><span class="p">(</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">filtered_sentences</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>

<span class="c1"># Print 5 most common words</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Most common words:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">frequency</span> <span class="ow">in</span> <span class="n">word_freq</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">frequency</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Summarize: Print the first 5 sentences as a simple summary</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Summary of the content:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
</code></pre></div>
<p>This code example includes:</p>
<ol>
<li>
<p><strong>HTML Content Cleaning</strong>: Using Beautiful Soup to parse the HTML content and remove unwanted script and style elements, making the text cleaner for analysis.</p>
</li>
<li>
<p><strong>Extracting Specific Information</strong>: Demonstrating how to extract and print hyperlinks and headings (e.g., h1 tags) from the web page. This can be adapted to extract other types of information as needed, such as images or specific sections.</p>
</li>
<li>
<p><strong>Text Summarization Basics</strong>: The code tokenizes the cleaned text into sentences, filters out stopwords to remove common but unimportant words, and calculates the frequency distribution of words. It then prints the most common words and uses the first few sentences to provide a simple summary of the content. For more advanced summarization, additional NLP techniques and models would be needed.</p>
</li>
</ol>
<p>This functionality demonstrates a foundational approach to processing and analyzing web content programmatically, from cleaning and information extraction to basic summarization.</p>
<h3 id="interacting-with-notion-data">Interacting with Notion Data</h3>
<p>Notion databases provide a structured format for personal and company data. The example below illustrates how to load data from a Notion database exported as Markdown.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.document_loaders</span><span class="w"> </span><span class="kn">import</span> <span class="n">NotionDirectoryLoader</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">markdown</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">bs4</span><span class="w"> </span><span class="kn">import</span> <span class="n">BeautifulSoup</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="c1"># Specify the directory containing the exported Notion data</span>
<span class="n">notion_directory</span> <span class="o">=</span> <span class="s2">&quot;docs/Notion_DB&quot;</span>

<span class="c1"># Initialize the Notion Directory Loader</span>
<span class="n">notion_loader</span> <span class="o">=</span> <span class="n">NotionDirectoryLoader</span><span class="p">(</span><span class="n">notion_directory</span><span class="p">)</span>

<span class="c1"># Load the documents</span>
<span class="n">notion_documents</span> <span class="o">=</span> <span class="n">notion_loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1"># Example: Printing the first 200 characters of a Notion document&#39;s content</span>
<span class="nb">print</span><span class="p">(</span><span class="n">notion_documents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="p">[:</span><span class="mi">200</span><span class="p">])</span>

<span class="c1"># Accessing the metadata of the Notion document</span>
<span class="nb">print</span><span class="p">(</span><span class="n">notion_documents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span><span class="p">)</span>

<span class="c1"># Convert Markdown to HTML for easier parsing and extraction</span>
<span class="n">html_content</span> <span class="o">=</span> <span class="p">[</span><span class="n">markdown</span><span class="o">.</span><span class="n">markdown</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">page_content</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">notion_documents</span><span class="p">]</span>

<span class="c1"># Parse HTML to extract structured data (e.g., headings, lists, links)</span>
<span class="n">parsed_data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">content</span> <span class="ow">in</span> <span class="n">html_content</span><span class="p">:</span>
    <span class="n">soup</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">content</span><span class="p">,</span> <span class="s1">&#39;html.parser&#39;</span><span class="p">)</span>
    <span class="c1"># Example: Extracting all headings (h1, h2, etc.)</span>
    <span class="n">headings</span> <span class="o">=</span> <span class="p">[</span><span class="n">heading</span><span class="o">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">heading</span> <span class="ow">in</span> <span class="n">soup</span><span class="o">.</span><span class="n">find_all</span><span class="p">([</span><span class="s1">&#39;h1&#39;</span><span class="p">,</span> <span class="s1">&#39;h2&#39;</span><span class="p">,</span> <span class="s1">&#39;h3&#39;</span><span class="p">,</span> <span class="s1">&#39;h4&#39;</span><span class="p">,</span> <span class="s1">&#39;h5&#39;</span><span class="p">,</span> <span class="s1">&#39;h6&#39;</span><span class="p">])]</span>
    <span class="c1"># Example: Extracting all links</span>
    <span class="n">links</span> <span class="o">=</span> <span class="p">[(</span><span class="n">a</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">a</span><span class="p">[</span><span class="s1">&#39;href&#39;</span><span class="p">])</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">soup</span><span class="o">.</span><span class="n">find_all</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="n">href</span><span class="o">=</span><span class="kc">True</span><span class="p">)]</span>
    <span class="n">parsed_data</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;headings&#39;</span><span class="p">:</span> <span class="n">headings</span><span class="p">,</span> <span class="s1">&#39;links&#39;</span><span class="p">:</span> <span class="n">links</span><span class="p">})</span>

<span class="c1"># Organize data into a DataFrame for further analysis</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;metadata&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">metadata</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">notion_documents</span><span class="p">],</span>
    <span class="s1">&#39;parsed_content&#39;</span><span class="p">:</span> <span class="n">parsed_data</span>
<span class="p">})</span>

<span class="c1"># Example of filtering: Find documents with specific keywords in metadata</span>
<span class="n">keyword</span> <span class="o">=</span> <span class="s1">&#39;Project&#39;</span>
<span class="n">filtered_docs</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;metadata&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">keyword</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;title&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">())]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Documents containing the keyword in the title:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">filtered_docs</span><span class="p">)</span>

<span class="c1"># Summarizing or generating reports based on the aggregated content</span>
<span class="c1"># Example: Counting documents by category (assuming categories are part of metadata)</span>
<span class="k">if</span> <span class="s1">&#39;category&#39;</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;metadata&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>  <span class="c1"># Check if category exists in metadata</span>
    <span class="n">category_counts</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;metadata&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;category&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Document counts by category:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">category_counts</span><span class="p">)</span>

<span class="c1"># This is a foundational approach to processing and analyzing Notion exported data.</span>
<span class="c1"># It demonstrates how to parse, filter, and summarize the content for insights or reporting.</span>
</code></pre></div>
<p>This example includes the following steps:</p>
<ol>
<li>
<p><strong>Markdown to HTML Conversion</strong>: For easier content parsing, the Markdown content of each Notion document is converted to HTML.</p>
</li>
<li>
<p><strong>Extracting Structured Data</strong>: Using Beautiful Soup to parse the HTML and extract structured data such as headings and links from each document.</p>
</li>
<li>
<p><strong>Organizing Data with Pandas</strong>: Creating a pandas DataFrame to organize the metadata and parsed content, facilitating further analysis and manipulation.</p>
</li>
<li>
<p><strong>Filtering and Analyzing Data</strong>: Demonstrating how to filter documents based on specific keywords in their metadata and how to categorize documents by metadata attributes (e.g., category), if available.</p>
</li>
<li>
<p><strong>Summarizing Data</strong>: Providing examples of how to summarize or generate reports from the data, such as counting documents by category.</p>
</li>
</ol>
<p>This approach offers a comprehensive method for handling and deriving insights from Notion database exports, leveraging Python's powerful data manipulation and analysis libraries.</p>
<h2 id="best-practices-and-tips">Best Practices and Tips</h2>
<ul>
<li><strong>Optimize API Usage</strong>: When working with external APIs, such as OpenAI's Whisper for transcription, monitor usage to avoid unexpected costs.</li>
<li><strong>Data Preprocessing</strong>: After loading data, it may require preprocessing (e.g., removing white space, splitting text) to be in a usable format for further analysis or model training.</li>
<li><strong>Contribute to Open Source</strong>: If you encounter data sources not supported</li>
</ul>
<p>by existing document loaders, consider contributing to the LangChain project by developing new loaders.</p>
<h2 id="further-reading-and-resources">Further Reading and Resources</h2>
<ul>
<li>LangChain Documentation: <a href="https://github.com/LangChain/langchain">LangChain GitHub Repository</a></li>
<li>OpenAI Whisper Model: <a href="https://github.com/openai/whisper">OpenAI Whisper GitHub Repository</a></li>
</ul>
<p>This guidebook chapter provides a foundational understanding of loading documents from various sources, setting the stage for more advanced data interaction and manipulation techniques.</p>
<h2 id="theory-questions">Theory questions:</h2>
<ol>
<li>What are document loaders in LangChain and what role do they play in data-driven applications?</li>
<li>How do unstructured data loaders differ from structured data loaders in LangChain?</li>
<li>Describe the process of setting up and configuring the environment to use LangChain document loaders.</li>
<li>How does the PyPDFLoader work in LangChain to load and process PDF documents?</li>
<li>Explain the significance of text cleaning and tokenization in processing PDF documents.</li>
<li>What are the steps involved in transcribing YouTube videos using LangChain and OpenAI's Whisper model?</li>
<li>Describe how sentence tokenization and sentiment analysis can be applied to transcribed YouTube video content.</li>
<li>How can web content be loaded and processed using the WebBaseLoader in LangChain?</li>
<li>Explain the process of extracting and summarizing content from URLs with LangChain.</li>
<li>How does the NotionDirectoryLoader facilitate loading and analyzing data from Notion databases in LangChain?</li>
<li>What best practices should be followed when using document loaders in LangChain for data processing and analysis?</li>
<li>Discuss the potential benefits of contributing new document loaders to the LangChain project.</li>
</ol>
<h2 id="practice-questions">Practice questions:</h2>
<ol>
<li>
<p><strong>PDF Document Word Frequency Analysis</strong>: Modify the given PDF document loading and word frequency analysis example to ignore common stopwords (e.g., "the", "is", "in"). Use the <code>nltk</code> library to filter out these stopwords from the analysis. Print the top 5 most common words that are not stopwords.</p>
</li>
<li>
<p><strong>Transcribing YouTube Video</strong>: Assuming you have a YouTube video URL, write a Python function that takes the URL as an input, uses the OpenAI Whisper model to transcribe the video, and returns the first 100 words of the transcription. Handle any potential errors gracefully.</p>
</li>
<li>
<p><strong>Loading and Cleaning Web Content</strong>: Given a URL, write a Python script that loads the web page content, removes all HTML tags, and prints the clean text. Use <code>BeautifulSoup</code> for HTML parsing and cleaning.</p>
</li>
<li>
<p><strong>Notion Data Analysis</strong>: Assuming you have a directory containing Notion data exported as Markdown files, write a Python script that converts all Markdown files to HTML, extracts all links (both the text and the href attribute), and prints them. Use the <code>markdown</code> library for conversion and <code>BeautifulSoup</code> for parsing the HTML.</p>
</li>
<li>
<p><strong>Sentiment Analysis on Transcribed Content</strong>: Extend the YouTube video transcription example by performing sentiment analysis on the transcribed text using the <code>TextBlob</code> library. Print out the overall sentiment score (polarity) and whether the content is mostly positive, negative, or neutral.</p>
</li>
<li>
<p><strong>Data Frame Manipulation</strong>: Based on the Notion data loading and processing example, write a Python script that creates a pandas DataFrame from the loaded Notion documents, adds a new column indicating the word count of each document's content, and prints the titles of the top 3 longest documents.</p>
</li>
<li>
<p><strong>Summarize Web Content</strong>: For a given URL, write a Python script that loads the web page, extracts the main content, and generates a simple summary by printing the first and last sentence of the content. Use <code>BeautifulSoup</code> for content extraction and <code>nltk</code> for sentence tokenization.</p>
</li>
</ol>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
    
  </body>
</html>