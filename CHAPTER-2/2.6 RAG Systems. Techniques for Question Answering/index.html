
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.17">
    
    
      
        <title>2.6 RAG Systems. Techniques for Question Answering - LLMOps. Make AI Work For You.</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#26-rag-systems-techniques-for-question-answering" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="LLMOps. Make AI Work For You." class="md-header__button md-logo" aria-label="LLMOps. Make AI Work For You." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LLMOps. Make AI Work For You.
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2.6 RAG Systems. Techniques for Question Answering
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="Select language">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="../../en/" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="../../ru/" hreflang="ru" class="md-select__link">
              Русский
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/boramorka/LLM-book" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    boramorka/LLM-book
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="LLMOps. Make AI Work For You." class="md-nav__button md-logo" aria-label="LLMOps. Make AI Work For You." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    LLMOps. Make AI Work For You.
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/boramorka/LLM-book" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    boramorka/LLM-book
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="26-rag-systems-techniques-for-question-answering">2.6 RAG Systems. Techniques for Question Answering</h1>
<h2 id="introduction">Introduction</h2>
<p>Retrieval Augmented Generation (RAG) systems have revolutionized the way we interact with large corpora of data, enabling the development of sophisticated chatbots and question-answering models. A critical stage in these systems involves passing retrieved documents, along with the original query, to a language model (LM) for generating answers. This chapter explores various strategies for optimizing this process, ensuring accurate and comprehensive responses.</p>
<h2 id="question-answering-with-language-models">Question Answering with Language Models</h2>
<p>Once relevant documents are retrieved, they must be effectively synthesized into coherent answers. This involves the integration of document content with the query context and leveraging the capabilities of LMs.</p>
<h3 id="general-flow">General Flow</h3>
<ol>
<li><strong>Query Reception</strong>: A user query is received.</li>
<li><strong>Document Retrieval</strong>: Relevant documents are sourced from the corpus.</li>
<li><strong>Answer Generation</strong>: Documents and the query are passed to an LM, which generates an answer.</li>
</ol>
<h3 id="integration-methods">Integration Methods</h3>
<p>By default, all retrieved chunks are passed into the LM's context window. However, limitations arise due to the context window size. Strategies like MapReduce, Refine, and MapRerank offer solutions to this constraint.</p>
<h2 id="enhancing-rag-systems-with-advanced-question-answering-techniques">Enhancing RAG Systems with Advanced Question Answering Techniques</h2>
<p>Before diving into the specifics of question answering with LMs, ensure your development environment is configured correctly. This setup includes importing necessary libraries, setting up API keys, and adjusting for any deprecations in LM versions.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dotenv</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">datetime</span>

<span class="c1"># Load environment variables and configure OpenAI API key</span>
<span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>

<span class="c1"># Adjust for LLM versioning</span>
<span class="n">current_date</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">date</span><span class="p">()</span>
<span class="n">llm_name</span> <span class="o">=</span> <span class="s2">&quot;gpt-3.5-turbo&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using LLM Version: </span><span class="si">{</span><span class="n">llm_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<h2 id="document-retrieval-with-vectordb">Document Retrieval with VectorDB</h2>
<p>A crucial step in RAG systems is retrieving documents relevant to a user's query. This is achieved using a vector database (VectorDB) that stores document embeddings.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Import necessary libraries for vector database and embeddings generation</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.vectorstores</span><span class="w"> </span><span class="kn">import</span> <span class="n">Chroma</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>

<span class="c1"># Specify the directory where the vector database will persist its data</span>
<span class="n">documents_storage_directory</span> <span class="o">=</span> <span class="s1">&#39;docs/chroma/&#39;</span>

<span class="c1"># Initialize the embeddings generator using OpenAI&#39;s embeddings</span>
<span class="n">embeddings_generator</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">()</span>

<span class="c1"># Initialize the vector database with the specified storage directory and embedding function</span>
<span class="n">vector_database</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">(</span><span class="n">persist_directory</span><span class="o">=</span><span class="n">documents_storage_directory</span><span class="p">,</span> <span class="n">embedding_function</span><span class="o">=</span><span class="n">embeddings_generator</span><span class="p">)</span>

<span class="c1"># Display the current document count in the vector database to verify initialization</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Document Count in VectorDB: </span><span class="si">{</span><span class="n">vector_database</span><span class="o">.</span><span class="n">_collection</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<h2 id="implementing-question-answering-chains">Implementing Question Answering Chains</h2>
<p>The RetrievalQA chain is a method that combines document retrieval with question answering, utilizing the capabilities of LMs to generate responses based on the retrieved documents.</p>
<h3 id="initializing-the-language-model">Initializing the Language Model</h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="c1"># Initialize the chat model with the chosen LLM version</span>
<span class="n">language_model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">llm_name</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<h3 id="configuring-the-retrievalqa-chain">Configuring the RetrievalQA Chain</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Importing necessary modules from the langchain library</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.chains</span><span class="w"> </span><span class="kn">import</span> <span class="n">RetrievalQA</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="c1"># Creating a custom prompt template for the language model</span>
<span class="c1"># The template guides the model to use the provided context effectively to answer the question</span>
<span class="n">custom_prompt_template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;To better assist with the inquiry, consider the details provided below as your reference...</span>
<span class="si">{context}</span>
<span class="s2">Inquiry: </span><span class="si">{question}</span>
<span class="s2">Insightful Response:&quot;&quot;&quot;</span>

<span class="c1"># Initializing the RetrievalQA chain with the custom prompt template</span>
<span class="n">question_answering_chain</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="o">.</span><span class="n">from_chain_type</span><span class="p">(</span>
    <span class="n">language_model</span><span class="p">,</span>
    <span class="n">retriever</span><span class="o">=</span><span class="n">vector_database</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(),</span>
    <span class="n">return_source_documents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">chain_type_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">PromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">custom_prompt_template</span><span class="p">)}</span>
<span class="p">)</span>
</code></pre></div>
<h3 id="question-answering-in-action">Question Answering in Action</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Pose a query to the system</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;Is probability a class topic?&quot;</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">qa_chain</span><span class="p">({</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Answer:&quot;</span><span class="p">,</span> <span class="n">response</span><span class="p">[</span><span class="s2">&quot;result&quot;</span><span class="p">])</span>
</code></pre></div>
<h2 id="exploring-advanced-qa-chain-types">Exploring Advanced QA Chain Types</h2>
<h3 id="mapreduce-and-refine-techniques">MapReduce and Refine Techniques</h3>
<p>MapReduce and Refine are advanced techniques designed to circumvent limitations posed by the LM's context window size, enabling the processing of numerous documents.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Configuring the question answering chain to use the MapReduce technique</span>
<span class="c1"># This configuration enables the aggregation of responses from multiple documents</span>
<span class="n">question_answering_chain_map_reduce</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="o">.</span><span class="n">from_chain_type</span><span class="p">(</span>
    <span class="n">language_model</span><span class="p">,</span>
    <span class="n">retriever</span><span class="o">=</span><span class="n">vector_database</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(),</span>
    <span class="n">chain_type</span><span class="o">=</span><span class="s2">&quot;map_reduce&quot;</span>
<span class="p">)</span>

<span class="c1"># Executing the MapReduce technique with a user-provided query</span>
<span class="n">response_map_reduce</span> <span class="o">=</span> <span class="n">question_answering_chain_map_reduce</span><span class="p">({</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">})</span>

<span class="c1"># Printing the aggregated answer obtained through the MapReduce technique</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MapReduce Answer:&quot;</span><span class="p">,</span> <span class="n">response_map_reduce</span><span class="p">[</span><span class="s2">&quot;result&quot;</span><span class="p">])</span>

<span class="c1"># Configuring the question answering chain to use the Refine technique</span>
<span class="c1"># This approach allows for the sequential refinement of the answer based on the query</span>
<span class="n">question_answering_chain_refine</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="o">.</span><span class="n">from_chain_type</span><span class="p">(</span>
    <span class="n">language_model</span><span class="p">,</span>
    <span class="n">retriever</span><span class="o">=</span><span class="n">vector_database</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(),</span>
    <span class="n">chain_type</span><span class="o">=</span><span class="s2">&quot;refine&quot;</span>
<span class="p">)</span>

<span class="c1"># Executing the Refine technique with the same user-provided query</span>
<span class="n">response_refine</span> <span class="o">=</span> <span class="n">question_answering_chain_refine</span><span class="p">({</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">})</span>

<span class="c1"># Printing the refined answer, showcasing the iterative improvement process</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Refine Answer:&quot;</span><span class="p">,</span> <span class="n">response_refine</span><span class="p">[</span><span class="s2">&quot;result&quot;</span><span class="p">])</span>
</code></pre></div>
<h4 id="practical-tips-and-best-practices">Practical Tips and Best Practices</h4>
<ul>
<li>
<p><strong>Choosing Between MapReduce and Refine</strong>: The decision to use MapReduce or Refine depends on the specific requirements of your task. MapReduce is best suited for scenarios where the goal is to aggregate information from multiple sources quickly. Refine, however, is more appropriate for tasks requiring high accuracy and the iterative improvement of answers.</p>
</li>
<li>
<p><strong>Optimizing Performance</strong>: When implementing these techniques, especially in distributed systems, pay attention to network latency and data serialization costs. Efficient data transfer and processing can significantly impact the overall performance.</p>
</li>
<li>
<p><strong>Experimentation is Key</strong>: The effectiveness of MapReduce and Refine can vary based on the nature of the data and the specifics of the question answering task. It's essential to experiment with both techniques to determine which yields the best results for your particular application.</p>
</li>
</ul>
<h2 id="addressing-retrievalqa-limitations">Addressing RetrievalQA Limitations</h2>
<p>A notable limitation of RetrievalQA chains is their inability to preserve conversational history, impacting the flow of follow-up queries.</p>
<h3 id="demonstrating-the-limitation">Demonstrating the Limitation</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Importing the question answering chain from a hypothetical library</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">some_library</span><span class="w"> </span><span class="kn">import</span> <span class="n">question_answering_chain</span> <span class="k">as</span> <span class="n">qa_chain</span>

<span class="c1"># Defining an initial query related to course content</span>
<span class="n">initial_question_about_course_content</span> <span class="o">=</span> <span class="s2">&quot;Does the curriculum cover probability theory?&quot;</span>
<span class="c1"># Generating a response to the initial query using the question answering chain</span>
<span class="n">response_to_initial_question</span> <span class="o">=</span> <span class="n">qa_chain</span><span class="p">({</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="n">initial_question_about_course_content</span><span class="p">})</span>

<span class="c1"># Defining a follow-up query without explicitly preserving the conversational context</span>
<span class="n">follow_up_question_about_prerequisites</span> <span class="o">=</span> <span class="s2">&quot;Why are those prerequisites important?&quot;</span>
<span class="c1"># Generating a response to the follow-up query, again using the question answering chain</span>
<span class="n">response_to_follow_up_question</span> <span class="o">=</span> <span class="n">qa_chain</span><span class="p">({</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="n">follow_up_question_about_prerequisites</span><span class="p">})</span>

<span class="c1"># Displaying the responses to both the initial and follow-up queries</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Response to Initial Query:&quot;</span><span class="p">,</span> <span class="n">response_to_initial_question</span><span class="p">[</span><span class="s2">&quot;result&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Response to Follow-Up Query:&quot;</span><span class="p">,</span> <span class="n">response_to_follow_up_question</span><span class="p">[</span><span class="s2">&quot;result&quot;</span><span class="p">])</span>
</code></pre></div>
<p>This limitation underscores the need for integrating conversational memory into RAG systems, a topic that will be explored in subsequent sections.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Advanced question answering</p>
<p>techniques in RAG systems offer a pathway to more dynamic and accurate responses, enhancing user interaction. Through the careful implementation of RetrievalQA chains, and by addressing inherent limitations, developers can create sophisticated systems capable of engaging in meaningful dialogues with users.</p>
<h2 id="further-reading-and-exploration">Further Reading and Exploration</h2>
<ul>
<li>Explore the latest advancements in language model technologies and their implications for RAG systems.</li>
<li>Investigate additional strategies for integrating conversational memory into RAG frameworks.</li>
</ul>
<p>This chapter provides a foundation for understanding and implementing advanced question answering techniques within RAG systems, setting the stage for further innovation in the field of AI-driven interaction.</p>
<h2 id="theory-questions">Theory questions:</h2>
<ol>
<li>What are the three main stages involved in the question answering process of a RAG system?</li>
<li>Describe the limitations of passing all retrieved chunks into the LM's context window and mention at least two strategies to overcome this constraint.</li>
<li>Explain the significance of using a vector database (VectorDB) in document retrieval for RAG systems.</li>
<li>How does the RetrievalQA chain combine document retrieval with question answering in RAG systems?</li>
<li>Compare and contrast the MapReduce and Refine techniques in the context of overcoming LM's context window size limitations.</li>
<li>What practical considerations should be taken into account when implementing MapReduce or Refine techniques in a distributed system?</li>
<li>Why is it crucial to experiment with both MapReduce and Refine techniques in a RAG system?</li>
<li>Identify a major limitation of RetrievalQA chains concerning conversational history and its impact on follow-up queries.</li>
<li>Discuss the importance of integrating conversational memory into RAG systems and how it could potentially enhance user interaction.</li>
<li>What are the recommended areas for further reading and exploration to advance one's understanding of RAG systems and their capabilities?</li>
</ol>
<h2 id="practice-questions">Practice questions:</h2>
<p>Based on the content of the chapter on advanced question answering techniques in RAG systems, here are some Python tasks that align with the key concepts and code examples presented:</p>
<ol>
<li><strong>Vector Database Initialization</strong></li>
<li>
<p>Implement a Python function that initializes a vector database for document retrieval. Use the Chroma class for the database and OpenAIEmbeddings for generating embeddings. The function should take a directory path as an input for where the vector database will store its data and print the current document count in the database.</p>
</li>
<li>
<p><strong>RetrievalQA Chain Setup</strong></p>
</li>
<li>
<p>Create a Python function that sets up a RetrievalQA chain with a custom prompt template. The function should initialize a language model and a vector database retriever, then configure the RetrievalQA chain using these components. Use the custom prompt template provided in the chapter, and allow the function to accept a model name and a documents storage directory as parameters.</p>
</li>
<li>
<p><strong>Question Answering with MapReduce and Refine Techniques</strong></p>
</li>
<li>
<p>Write a Python script that demonstrates the use of MapReduce and Refine techniques for question answering. Your script should include the initialization of language model and vector database components, setup for both MapReduce and Refine question answering chains, and execute these chains with a sample query. Print the results of both techniques.</p>
</li>
<li>
<p><strong>Handling Conversational Context</strong></p>
</li>
<li>Implement a Python function that simulates the handling of a follow-up question in a conversational context. The function should accept two queries (an initial query and a follow-up query) and generate responses to both using a question answering chain. This task aims to illustrate the limitation mentioned in the chapter regarding the preservation of conversational history. Your implementation does not need to solve the limitation but should demonstrate how the system currently handles follow-up queries.</li>
</ol>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
    
  </body>
</html>